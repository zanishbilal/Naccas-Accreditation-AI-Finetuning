{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "wcttmyDOLa6z",
        "CYgm6hWPTW_T",
        "HhP32UuhMd7O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8880c5bdb0b94406baa6e8c11c2b4b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d46e2dd0f90460aaa697121d99e0bf8",
              "IPY_MODEL_25f17d9ed7f647a2a509127d8ad4613f",
              "IPY_MODEL_c17bfdc5801c4fd7a4b1dfae84079651"
            ],
            "layout": "IPY_MODEL_b068f9f0d26d4c3b90db39c62e33b93d"
          }
        },
        "8d46e2dd0f90460aaa697121d99e0bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6fcc9514be4c27bdac719631feb87f",
            "placeholder": "​",
            "style": "IPY_MODEL_c7cc96d21d5e4e6697ed42a46c705ab2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "25f17d9ed7f647a2a509127d8ad4613f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665636d48b8a41e3842c3da4ad2c516c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f164bdfa58e64095a60e8af4fc45465d",
            "value": 2
          }
        },
        "c17bfdc5801c4fd7a4b1dfae84079651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad56c5f41315422988b36848c624ace7",
            "placeholder": "​",
            "style": "IPY_MODEL_5308eb24baa443e2a4bf9a5f6110acb9",
            "value": " 2/2 [00:30&lt;00:00, 13.98s/it]"
          }
        },
        "b068f9f0d26d4c3b90db39c62e33b93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6fcc9514be4c27bdac719631feb87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7cc96d21d5e4e6697ed42a46c705ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665636d48b8a41e3842c3da4ad2c516c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f164bdfa58e64095a60e8af4fc45465d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad56c5f41315422988b36848c624ace7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5308eb24baa443e2a4bf9a5f6110acb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c7de962b8bb42afb2b38a4501216ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_678958cec02c432abbc33bb4b31b3ecf",
              "IPY_MODEL_0fe11ee33bed4af7be9a79ae6d0c72c0",
              "IPY_MODEL_4718dcdcd78c4f05812c56f4bde3ff15"
            ],
            "layout": "IPY_MODEL_8176f495ac874e34b9e877e315045a7b"
          }
        },
        "678958cec02c432abbc33bb4b31b3ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22138cc16e334027a55f77ff83903e4d",
            "placeholder": "​",
            "style": "IPY_MODEL_15d31e8d6b064248be42e071c3408f37",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0fe11ee33bed4af7be9a79ae6d0c72c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0c6be6e0484b0f8ee41d5f14fc7333",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_785d4fe3d2644ae795ee5e4af45cea2e",
            "value": 2
          }
        },
        "4718dcdcd78c4f05812c56f4bde3ff15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f958024ab52d4d3aacd4ba40b2ec1cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_59877dd24f8d42afaa3efed1a8e54a22",
            "value": " 2/2 [00:19&lt;00:00,  9.66s/it]"
          }
        },
        "8176f495ac874e34b9e877e315045a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22138cc16e334027a55f77ff83903e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d31e8d6b064248be42e071c3408f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b0c6be6e0484b0f8ee41d5f14fc7333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "785d4fe3d2644ae795ee5e4af45cea2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f958024ab52d4d3aacd4ba40b2ec1cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59877dd24f8d42afaa3efed1a8e54a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#V2 inference"
      ],
      "metadata": {
        "id": "wcttmyDOLa6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes pyngrok\n",
        "\n",
        "# =========================\n",
        "# ✅ IMPORTS\n",
        "# =========================\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login\n",
        "import zipfile\n",
        "import os\n",
        "import gc\n",
        "# =========================\n",
        "# ✅ AUTHENTICATION\n",
        "# =========================\n",
        "login(token=\"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\")  # Replace securely in production\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQWe8S0CLaoF",
        "outputId": "b629c6da-1bf4-41c8-a60d-d1fa5b1aa6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ FILE EXTRACTION\n",
        "# =========================\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Extraction complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IV-CmLeLrQ-",
        "outputId": "3a6b3038-44eb-40f8-8885-7f110d6e9e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# ✅ PATHS & CONFIGS\n",
        "# =========================\n",
        "base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# ✅ LOAD MODEL & TOKENIZER\n",
        "# =========================\n",
        "def load_model_and_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "# =========================\n",
        "# ✅ INFERENCE FUNCTION\n",
        "# =========================\n",
        "def generate_response(prompt, max_new_tokens=80):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "xradrsRjLxoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ EXAMPLE RUN\n",
        "# =========================\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "prompt = \"You are a NACCAS policy expert. Answer using only official NACCAS documentation. can a student with ged get admission in NACCAS accredited institute?\"\n",
        "response = generate_response(prompt)\n",
        "# Split into sentences and keep first 3\n",
        "sentences = response.strip().split(\".\")\n",
        "short_response = \".\".join(sentences[:3]).strip() + \".\"\n",
        "print(\"🧠 Model response:\\n\", short_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "8880c5bdb0b94406baa6e8c11c2b4b05",
            "8d46e2dd0f90460aaa697121d99e0bf8",
            "25f17d9ed7f647a2a509127d8ad4613f",
            "c17bfdc5801c4fd7a4b1dfae84079651",
            "b068f9f0d26d4c3b90db39c62e33b93d",
            "7b6fcc9514be4c27bdac719631feb87f",
            "c7cc96d21d5e4e6697ed42a46c705ab2",
            "665636d48b8a41e3842c3da4ad2c516c",
            "f164bdfa58e64095a60e8af4fc45465d",
            "ad56c5f41315422988b36848c624ace7",
            "5308eb24baa443e2a4bf9a5f6110acb9"
          ]
        },
        "id": "mg25KBfqL0ak",
        "outputId": "3b5bf32a-f3c2-482c-b131-7b5675e34a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8880c5bdb0b94406baa6e8c11c2b4b05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Model response:\n",
            " A copy of the student?s high school transcript or certificate must be maintained\n",
            "on file at the institution and a copy is to be sent to the student?s home address.\n",
            "33. If an institution becomes subject to one or more Show/Cause Orders, the school shall\n",
            "send a copy to all enrolled students.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V2.5 inference (not useful right now)"
      ],
      "metadata": {
        "id": "CYgm6hWPTW_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19QjBrX77-ET",
        "outputId": "d81658bf-64ff-4ab5-9f8a-fce8c07e10d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# # =========================\n",
        "# # ✅ SETUP & DEPENDENCIES\n",
        "# # =========================\n",
        "# !pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "\n",
        "# # =========================\n",
        "# # ✅ IMPORTS\n",
        "# # =========================\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from peft import PeftModel\n",
        "# from pyngrok import ngrok\n",
        "# import zipfile\n",
        "# import os\n",
        "# from huggingface_hub import login\n",
        "# # %%writefile app.py\n",
        "# # import streamlit as st\n",
        "# # !ngrok config add-authtoken\n",
        "\n",
        "# # =========================\n",
        "# # ✅ AUTHENTICATION\n",
        "# # =========================\n",
        "# # Login to Hugging Face (you might want to handle this differently for security)\n",
        "# login(token=\"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\")  # Replace with your token or use environment variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # ✅ FILE EXTRACTION\n",
        "# # =========================\n",
        "# zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "# extract_path = \"/content\"\n",
        "\n",
        "# # Extract the zip\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(\"✅ Extraction complete!\")\n",
        "\n",
        "# # =========================\n",
        "# # ✅ PATHS & CONFIGS\n",
        "# # =========================\n",
        "# base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "# fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "# )"
      ],
      "metadata": {
        "id": "jyyQNb1KL_KS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3653b01a-e1dd-4d7d-ccf0-000e9123b1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # ✅ INFERENCE FUNCTION\n",
        "# # =========================\n",
        "\n",
        "# def generate_response(user_prompt, system_prompt, max_new_tokens=150):\n",
        "#     # Load model and tokenizer\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "#     # Create properly formatted Llama 3 prompt\n",
        "#     prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "#     Cutting Knowledge Date: December 2023\n",
        "#     Today Date: 23 July 2024\n",
        "#     {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "#     {user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "#     # Tokenize and move to model's device\n",
        "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     # Generate response\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,\n",
        "#             top_k=20,\n",
        "#             top_p=0.9,\n",
        "#             repetition_penalty=1.2,\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     # Decode only the assistant's response (skip the input prompt)\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "#     return response.strip()"
      ],
      "metadata": {
        "id": "iEvlDqdX9lrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # =========================\n",
        "# # # ✅ TEST PROMPT\n",
        "# # # =========================\n",
        "# # prompt = \"What must a NACCAS-accredited institute include in its school catalog regarding its Ability to Benefit policy?\"\n",
        "# system_prompt = \"You are a NACCAS policy expert. Answer only using official documents.\"\n",
        "# user_prompt = \"What must NACCAS accredited institute include in its school catalog regarding its ability to benifit policy?.\"\n",
        "\n",
        "# response = generate_response(user_prompt,system_prompt)\n",
        "# print(\"🧠 Model response:\\n\", response)"
      ],
      "metadata": {
        "id": "s-_bK6M892zK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "7c7de962b8bb42afb2b38a4501216ae6",
            "678958cec02c432abbc33bb4b31b3ecf",
            "0fe11ee33bed4af7be9a79ae6d0c72c0",
            "4718dcdcd78c4f05812c56f4bde3ff15",
            "8176f495ac874e34b9e877e315045a7b",
            "22138cc16e334027a55f77ff83903e4d",
            "15d31e8d6b064248be42e071c3408f37",
            "2b0c6be6e0484b0f8ee41d5f14fc7333",
            "785d4fe3d2644ae795ee5e4af45cea2e",
            "f958024ab52d4d3aacd4ba40b2ec1cb3",
            "59877dd24f8d42afaa3efed1a8e54a22"
          ]
        },
        "outputId": "6ac92881-6770-4dc6-be95-0ed20be859ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c7de962b8bb42afb2b38a4501216ae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-3.2-3B', revision=None, inference_mode=True, r=32, target_modules={'up_proj', 'gate_proj', 'down_proj', 'v_proj', 'k_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
            "🧠 Model response:\n",
            " A copy of the current accreditation agreement between the institution and NACCCS is included in the catalog.圭圭ニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v2 + streamlit and ngrok both on colab"
      ],
      "metadata": {
        "id": "HhP32UuhMd7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ FILE EXTRACTION\n",
        "# =========================\n",
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "# Extract the zip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Extraction complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0pEr-kbMrG0",
        "outputId": "8cb17e70-fbf1-4009-a296-fafc8d3e8928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe05jDuSMjhz",
        "outputId": "2a6d71ce-5de2-41e2-cd46-85e08aab7d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ CREATE STREAMLIT APP FILE (in separate cell)\n",
        "# =========================\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "    fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(fine_tuned_dir, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model = model.merge_and_unload()\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=200):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "st.title(\"Llama-3 Policy Assistant\")\n",
        "prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "if st.button(\"Generate Response\"):\n",
        "    if prompt:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        response = generate_response(prompt)\n",
        "        # Split into sentences and keep first 10\n",
        "        sentences = response.strip().split(\".\")\n",
        "        short_response = \".\".join(sentences[:10]).strip() + \".\"\n",
        "        st.write(short_response)\n",
        "    else:\n",
        "        st.warning(\"Please enter a question\")\n",
        "\n",
        "  # COPY PASTE THIS BEFORE YOUR PROMPT. You are a NACCAS policy expert. Answer using only official NACCAS documentation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK1PnROdMdED",
        "outputId": "43ae2bc5-efe4-472f-ad60-a0ef4b8b32a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ MAIN EXECUTION (in separate cell)\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=\"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\")  # Replace with your token\n",
        "\n",
        "\n",
        "# Set ngrok authtoken\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")  # Replace with your token\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjLQ7TFEMpF_",
        "outputId": "37c40dd8-7352-4756-c7d2-ae229c5eba10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://9e86-34-105-83-124.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.105.83.124:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-05-21 17:37:14.544330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747849034.799655    1596 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747849034.862042    1596 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:37:15.406416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-21 17:37:21.950 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v3"
      ],
      "metadata": {
        "id": "xHuY5mbMbDfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Significantly better and coherent results. requires minor adjustment, still putting out tokens like < /s> or < /item> etc"
      ],
      "metadata": {
        "id": "L_rTOZjViAxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q unsloth peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "!pip install PyPDF2 docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaTm9wwpbFvE",
        "outputId": "fd3dc1f5-5c08-44e3-e4fb-e6d6c83a6d81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.2.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=53c5ea0fd15e1b6c1c7bf91e8b4365dfe35dc56ed776552528388386fa5f2d7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: PyPDF2, docx\n",
            "Successfully installed PyPDF2-3.0.1 docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMPLE STREAMLIT GUI\n",
        "\n",
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import torch\n",
        "# from unsloth import FastLanguageModel\n",
        "# from transformers import AutoTokenizer\n",
        "# import gc\n",
        "# import re\n",
        "\n",
        "# @st.cache_resource\n",
        "# def load_model_and_tokenizer():\n",
        "#     fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "\n",
        "#     # Load the fine-tuned model and tokenizer\n",
        "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#         model_name=fine_tuned_dir,\n",
        "#         load_in_4bit=True,\n",
        "#         max_seq_length=2048,\n",
        "#         device_map=\"auto\",\n",
        "#     )\n",
        "#     model.eval()\n",
        "#     return model, tokenizer\n",
        "\n",
        "# def clean_response(response):\n",
        "#     # Remove all HTML-like tags, special tokens, and normalize whitespace\n",
        "#     response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)  # Remove tags like <item>, </item>, <s>, </s>, <span>, [INST], etc.\n",
        "#     response = re.sub(r'\\s+', ' ', response).strip()  # Normalize whitespace\n",
        "#     # Split into sentences and take the first 10 valid sentences\n",
        "#     sentences = response.split('.')\n",
        "#     cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "#     return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "# def generate_response(prompt, max_new_tokens=100):\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# # ----------------------- Format the prompt with NACCAS context -------------------------\n",
        "#     formatted_prompt = (\n",
        "#         f\"<s>[INST] You are a NACCAS policy expert. Answer the following question based strictly on official NACCAS documentation: {prompt} [/INST]\"\n",
        "#     )\n",
        "#     input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,  # Lowered for less randomness\n",
        "#             top_k=10,        # Stricter top-k for coherence\n",
        "#             top_p=0.85,      # Adjusted for more focused sampling\n",
        "#             repetition_penalty=1.3,  # Increased to reduce repetition\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "#     return clean_response(response)\n",
        "\n",
        "# st.title(\"NACCAS Policy Assistant\")\n",
        "# prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "# if st.button(\"Generate Response\"):\n",
        "#     if prompt:\n",
        "#         gc.collect()\n",
        "#         torch.cuda.empty_cache()\n",
        "#         response = generate_response(prompt)\n",
        "#         st.write(response)\n",
        "#     else:\n",
        "#         st.warning(\"Please enter a question about NACCAS policies\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kScR0pugQLn",
        "outputId": "272271d1-c5af-488b-9f15-c1481603c06f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "try:\n",
        "    import docx\n",
        "except ImportError:\n",
        "    docx = None\n",
        "import uuid\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import re\n",
        "\n",
        "# Initialize session state\n",
        "if 'chat_sessions' not in st.session_state:\n",
        "    st.session_state.chat_sessions = {}\n",
        "if 'current_chat_id' not in st.session_state:\n",
        "    st.session_state.current_chat_id = str(uuid.uuid4())\n",
        "if 'memory' not in st.session_state:\n",
        "    st.session_state.memory = []\n",
        "\n",
        "# Load persistent state from JSON\n",
        "def load_state():\n",
        "    try:\n",
        "        with open(\"chat_state.json\", \"r\") as f:\n",
        "            state = json.load(f)\n",
        "            st.session_state.chat_sessions = {\n",
        "                k: v for k, v in state.get(\"chat_sessions\", {}).items()\n",
        "            }\n",
        "            st.session_state.memory = state.get(\"memory\", [])\n",
        "            st.session_state.current_chat_id = state.get(\"current_chat_id\", str(uuid.uuid4()))\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "# Save state to JSON\n",
        "def save_state():\n",
        "    with open(\"chat_state.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            \"chat_sessions\": st.session_state.chat_sessions,\n",
        "            \"memory\": st.session_state.memory,\n",
        "            \"current_chat_id\": st.session_state.current_chat_id\n",
        "        }, f)\n",
        "\n",
        "# Load state at startup\n",
        "load_state()\n",
        "\n",
        "# Load Unsloth model and tokenizer\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=fine_tuned_dir,\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# Clean response function\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)\n",
        "    response = re.sub(r'\\s+', ' ', response).strip()\n",
        "    sentences = response.split('.')\n",
        "    cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "# Generate response function\n",
        "def generate_response(prompt, chat_history):\n",
        "    context = \"\"\n",
        "    for mem in st.session_state.memory:\n",
        "        context += f\"[MEMORY]: {mem}\\n\"\n",
        "    for msg in chat_history:\n",
        "        context += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
        "    context += f\"User: {prompt}\\n\"\n",
        "\n",
        "    formatted_prompt = f\"<s>[INST] You are a NACCAS policy expert. Answer the following based strictly on official NACCAS documentation:\\n{context} [/INST]\"\n",
        "\n",
        "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.3,\n",
        "            top_k=10,\n",
        "            top_p=0.85,\n",
        "            repetition_penalty=1.3,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return clean_response(response)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_pdf_text(file):\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract text from Word document\n",
        "def extract_docx_text(file):\n",
        "    if docx is None:\n",
        "        return \"Word document support unavailable (python-docx not installed).\"\n",
        "    try:\n",
        "        doc = docx.Document(file)\n",
        "        text = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading Word document: {str(e)}\"\n",
        "\n",
        "# Function to process uploaded file\n",
        "def process_uploaded_file(uploaded_file):\n",
        "    if uploaded_file is not None:\n",
        "        if uploaded_file.name.endswith('.pdf'):\n",
        "            text = extract_pdf_text(uploaded_file)\n",
        "        elif uploaded_file.name.endswith('.docx'):\n",
        "            text = extract_docx_text(uploaded_file)\n",
        "        else:\n",
        "            text = \"Unsupported file format. Please upload a PDF or Word document.\"\n",
        "        return text\n",
        "    return None\n",
        "\n",
        "# Function to create a new chat session\n",
        "def new_chat():\n",
        "    new_chat_id = str(uuid.uuid4())\n",
        "    st.session_state.chat_sessions[new_chat_id] = []\n",
        "    st.session_state.current_chat_id = new_chat_id\n",
        "    if len(st.session_state.chat_sessions) > 5:\n",
        "        oldest_chat_id = next(iter(st.session_state.chat_sessions))\n",
        "        del st.session_state.chat_sessions[oldest_chat_id]\n",
        "    save_state()\n",
        "\n",
        "# Sidebar for chat session management\n",
        "st.sidebar.title(\"Chat Sessions\")\n",
        "if st.sidebar.button(\"New Chat\"):\n",
        "    new_chat()\n",
        "\n",
        "for chat_id in list(st.session_state.chat_sessions.keys())[:5]:\n",
        "    if st.sidebar.button(f\"Chat {chat_id[:8]}\", key=chat_id):\n",
        "        st.session_state.current_chat_id = chat_id\n",
        "        save_state()\n",
        "\n",
        "# Main chat interface\n",
        "st.title(\"NACCAS Policy Assistant\")\n",
        "\n",
        "if st.session_state.current_chat_id not in st.session_state.chat_sessions:\n",
        "    st.session_state.chat_sessions[st.session_state.current_chat_id] = []\n",
        "    save_state()\n",
        "\n",
        "# Display chat history\n",
        "st.subheader(\"Chat History\")\n",
        "for message in st.session_state.chat_sessions[st.session_state.current_chat_id]:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Display memory\n",
        "st.subheader(\"Memory\")\n",
        "if st.session_state.memory:\n",
        "    for mem in st.session_state.memory:\n",
        "        st.write(f\"- {mem}\")\n",
        "else:\n",
        "    st.write(\"No memory items yet.\")\n",
        "\n",
        "# Input form to prevent infinite reruns\n",
        "with st.form(key=\"input_form\", clear_on_submit=True):\n",
        "    col1, col2 = st.columns([1, 4])\n",
        "    with col1:\n",
        "        uploaded_file = st.file_uploader(\"Upload PDF/Word\", type=['pdf', 'docx'], label_visibility=\"collapsed\")\n",
        "    with col2:\n",
        "        user_input = st.text_input(\"Ask about NACCAS policies...\", key=\"user_input\")\n",
        "    submit_button = st.form_submit_button(\"Send\")\n",
        "\n",
        "# Process input and file on form submission\n",
        "if submit_button and (user_input or uploaded_file):\n",
        "    current_chat = st.session_state.chat_sessions[st.session_state.current_chat_id]\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if uploaded_file:\n",
        "        file_content = process_uploaded_file(uploaded_file)\n",
        "        if file_content:\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Uploaded file content:\\n{file_content}\"\n",
        "            })\n",
        "            st.session_state.memory.append(f\"Uploaded file: {uploaded_file.name}\")\n",
        "            if len(st.session_state.memory) > 5:\n",
        "                st.session_state.memory.pop(0)\n",
        "            assistant_response = generate_response(f\"User uploaded a file: {uploaded_file.name}. Content: {file_content}\", current_chat)\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": assistant_response\n",
        "            })\n",
        "            save_state()\n",
        "\n",
        "    if user_input:\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        })\n",
        "        assistant_response = generate_response(user_input, current_chat)\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_response\n",
        "        })\n",
        "        st.session_state.memory.append(f\"User asked: {user_input}\")\n",
        "        if len(st.session_state.memory) > 5:\n",
        "            st.session_state.memory.pop(0)\n",
        "        save_state()\n",
        "\n",
        "    st.rerun()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4aKE0HgGNu2",
        "outputId": "5954f0fa-a89c-4530-ccc3-6b7e72e4bae5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# ✅ MAIN EXECUTION\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face (replace with your token)\n",
        "login(token=\"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\")\n",
        "\n",
        "# Set ngrok authtoken (replace with your token)\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4Mm2ADgOjv",
        "outputId": "c57f20f0-df62-4993-b889-42784bdf4582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://ea69-34-125-99-167.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.99.167:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}