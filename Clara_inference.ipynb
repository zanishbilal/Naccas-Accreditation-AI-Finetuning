{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "wcttmyDOLa6z",
        "CYgm6hWPTW_T",
        "HhP32UuhMd7O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8880c5bdb0b94406baa6e8c11c2b4b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d46e2dd0f90460aaa697121d99e0bf8",
              "IPY_MODEL_25f17d9ed7f647a2a509127d8ad4613f",
              "IPY_MODEL_c17bfdc5801c4fd7a4b1dfae84079651"
            ],
            "layout": "IPY_MODEL_b068f9f0d26d4c3b90db39c62e33b93d"
          }
        },
        "8d46e2dd0f90460aaa697121d99e0bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6fcc9514be4c27bdac719631feb87f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c7cc96d21d5e4e6697ed42a46c705ab2",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "25f17d9ed7f647a2a509127d8ad4613f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665636d48b8a41e3842c3da4ad2c516c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f164bdfa58e64095a60e8af4fc45465d",
            "value": 2
          }
        },
        "c17bfdc5801c4fd7a4b1dfae84079651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad56c5f41315422988b36848c624ace7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5308eb24baa443e2a4bf9a5f6110acb9",
            "value": "â€‡2/2â€‡[00:30&lt;00:00,â€‡13.98s/it]"
          }
        },
        "b068f9f0d26d4c3b90db39c62e33b93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6fcc9514be4c27bdac719631feb87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7cc96d21d5e4e6697ed42a46c705ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665636d48b8a41e3842c3da4ad2c516c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f164bdfa58e64095a60e8af4fc45465d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad56c5f41315422988b36848c624ace7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5308eb24baa443e2a4bf9a5f6110acb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c7de962b8bb42afb2b38a4501216ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_678958cec02c432abbc33bb4b31b3ecf",
              "IPY_MODEL_0fe11ee33bed4af7be9a79ae6d0c72c0",
              "IPY_MODEL_4718dcdcd78c4f05812c56f4bde3ff15"
            ],
            "layout": "IPY_MODEL_8176f495ac874e34b9e877e315045a7b"
          }
        },
        "678958cec02c432abbc33bb4b31b3ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22138cc16e334027a55f77ff83903e4d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_15d31e8d6b064248be42e071c3408f37",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "0fe11ee33bed4af7be9a79ae6d0c72c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0c6be6e0484b0f8ee41d5f14fc7333",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_785d4fe3d2644ae795ee5e4af45cea2e",
            "value": 2
          }
        },
        "4718dcdcd78c4f05812c56f4bde3ff15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f958024ab52d4d3aacd4ba40b2ec1cb3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_59877dd24f8d42afaa3efed1a8e54a22",
            "value": "â€‡2/2â€‡[00:19&lt;00:00,â€‡â€‡9.66s/it]"
          }
        },
        "8176f495ac874e34b9e877e315045a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22138cc16e334027a55f77ff83903e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d31e8d6b064248be42e071c3408f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b0c6be6e0484b0f8ee41d5f14fc7333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "785d4fe3d2644ae795ee5e4af45cea2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f958024ab52d4d3aacd4ba40b2ec1cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59877dd24f8d42afaa3efed1a8e54a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#V2 inference"
      ],
      "metadata": {
        "id": "wcttmyDOLa6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes pyngrok\n",
        "\n",
        "# =========================\n",
        "# âœ… IMPORTS\n",
        "# =========================\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login\n",
        "import zipfile\n",
        "import os\n",
        "import gc\n",
        "# =========================\n",
        "# âœ… AUTHENTICATION\n",
        "# =========================\n",
        "login(token=\"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\")  # Replace securely in production\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQWe8S0CLaoF",
        "outputId": "b629c6da-1bf4-41c8-a60d-d1fa5b1aa6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… FILE EXTRACTION\n",
        "# =========================\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"âœ… Extraction complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IV-CmLeLrQ-",
        "outputId": "3a6b3038-44eb-40f8-8885-7f110d6e9e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# âœ… PATHS & CONFIGS\n",
        "# =========================\n",
        "base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# âœ… LOAD MODEL & TOKENIZER\n",
        "# =========================\n",
        "def load_model_and_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "# =========================\n",
        "# âœ… INFERENCE FUNCTION\n",
        "# =========================\n",
        "def generate_response(prompt, max_new_tokens=80):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "xradrsRjLxoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… EXAMPLE RUN\n",
        "# =========================\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "prompt = \"You are a NACCAS policy expert. Answer using only official NACCAS documentation. can a student with ged get admission in NACCAS accredited institute?\"\n",
        "response = generate_response(prompt)\n",
        "# Split into sentences and keep first 3\n",
        "sentences = response.strip().split(\".\")\n",
        "short_response = \".\".join(sentences[:3]).strip() + \".\"\n",
        "print(\"ğŸ§  Model response:\\n\", short_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "8880c5bdb0b94406baa6e8c11c2b4b05",
            "8d46e2dd0f90460aaa697121d99e0bf8",
            "25f17d9ed7f647a2a509127d8ad4613f",
            "c17bfdc5801c4fd7a4b1dfae84079651",
            "b068f9f0d26d4c3b90db39c62e33b93d",
            "7b6fcc9514be4c27bdac719631feb87f",
            "c7cc96d21d5e4e6697ed42a46c705ab2",
            "665636d48b8a41e3842c3da4ad2c516c",
            "f164bdfa58e64095a60e8af4fc45465d",
            "ad56c5f41315422988b36848c624ace7",
            "5308eb24baa443e2a4bf9a5f6110acb9"
          ]
        },
        "id": "mg25KBfqL0ak",
        "outputId": "3b5bf32a-f3c2-482c-b131-7b5675e34a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8880c5bdb0b94406baa6e8c11c2b4b05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Model response:\n",
            " A copy of the student?s high school transcript or certificate must be maintained\n",
            "on file at the institution and a copy is to be sent to the student?s home address.\n",
            "33. If an institution becomes subject to one or more Show/Cause Orders, the school shall\n",
            "send a copy to all enrolled students.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V2.5 inference (not useful right now)"
      ],
      "metadata": {
        "id": "CYgm6hWPTW_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19QjBrX77-ET",
        "outputId": "d81658bf-64ff-4ab5-9f8a-fce8c07e10d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# # =========================\n",
        "# # âœ… SETUP & DEPENDENCIES\n",
        "# # =========================\n",
        "# !pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "\n",
        "# # =========================\n",
        "# # âœ… IMPORTS\n",
        "# # =========================\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from peft import PeftModel\n",
        "# from pyngrok import ngrok\n",
        "# import zipfile\n",
        "# import os\n",
        "# from huggingface_hub import login\n",
        "# # %%writefile app.py\n",
        "# # import streamlit as st\n",
        "# # !ngrok config add-authtoken\n",
        "\n",
        "# # =========================\n",
        "# # âœ… AUTHENTICATION\n",
        "# # =========================\n",
        "# # Login to Hugging Face (you might want to handle this differently for security)\n",
        "# login(token=\"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\")  # Replace with your token or use environment variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # âœ… FILE EXTRACTION\n",
        "# # =========================\n",
        "# zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "# extract_path = \"/content\"\n",
        "\n",
        "# # Extract the zip\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(\"âœ… Extraction complete!\")\n",
        "\n",
        "# # =========================\n",
        "# # âœ… PATHS & CONFIGS\n",
        "# # =========================\n",
        "# base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "# fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "# )"
      ],
      "metadata": {
        "id": "jyyQNb1KL_KS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3653b01a-e1dd-4d7d-ccf0-000e9123b1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # âœ… INFERENCE FUNCTION\n",
        "# # =========================\n",
        "\n",
        "# def generate_response(user_prompt, system_prompt, max_new_tokens=150):\n",
        "#     # Load model and tokenizer\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "#     # Create properly formatted Llama 3 prompt\n",
        "#     prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "#     Cutting Knowledge Date: December 2023\n",
        "#     Today Date: 23 July 2024\n",
        "#     {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "#     {user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "#     # Tokenize and move to model's device\n",
        "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     # Generate response\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,\n",
        "#             top_k=20,\n",
        "#             top_p=0.9,\n",
        "#             repetition_penalty=1.2,\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     # Decode only the assistant's response (skip the input prompt)\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "#     return response.strip()"
      ],
      "metadata": {
        "id": "iEvlDqdX9lrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # =========================\n",
        "# # # âœ… TEST PROMPT\n",
        "# # # =========================\n",
        "# # prompt = \"What must a NACCAS-accredited institute include in its school catalog regarding its Ability to Benefit policy?\"\n",
        "# system_prompt = \"You are a NACCAS policy expert. Answer only using official documents.\"\n",
        "# user_prompt = \"What must NACCAS accredited institute include in its school catalog regarding its ability to benifit policy?.\"\n",
        "\n",
        "# response = generate_response(user_prompt,system_prompt)\n",
        "# print(\"ğŸ§  Model response:\\n\", response)"
      ],
      "metadata": {
        "id": "s-_bK6M892zK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "7c7de962b8bb42afb2b38a4501216ae6",
            "678958cec02c432abbc33bb4b31b3ecf",
            "0fe11ee33bed4af7be9a79ae6d0c72c0",
            "4718dcdcd78c4f05812c56f4bde3ff15",
            "8176f495ac874e34b9e877e315045a7b",
            "22138cc16e334027a55f77ff83903e4d",
            "15d31e8d6b064248be42e071c3408f37",
            "2b0c6be6e0484b0f8ee41d5f14fc7333",
            "785d4fe3d2644ae795ee5e4af45cea2e",
            "f958024ab52d4d3aacd4ba40b2ec1cb3",
            "59877dd24f8d42afaa3efed1a8e54a22"
          ]
        },
        "outputId": "6ac92881-6770-4dc6-be95-0ed20be859ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c7de962b8bb42afb2b38a4501216ae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-3.2-3B', revision=None, inference_mode=True, r=32, target_modules={'up_proj', 'gate_proj', 'down_proj', 'v_proj', 'k_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
            "ğŸ§  Model response:\n",
            " A copy of the current accreditation agreement between the institution and NACCCS is included in the catalog.åœ­åœ­ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v2 + streamlit and ngrok both on colab"
      ],
      "metadata": {
        "id": "HhP32UuhMd7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… FILE EXTRACTION\n",
        "# =========================\n",
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "# Extract the zip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"âœ… Extraction complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0pEr-kbMrG0",
        "outputId": "8cb17e70-fbf1-4009-a296-fafc8d3e8928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe05jDuSMjhz",
        "outputId": "2a6d71ce-5de2-41e2-cd46-85e08aab7d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… CREATE STREAMLIT APP FILE (in separate cell)\n",
        "# =========================\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "    fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(fine_tuned_dir, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model = model.merge_and_unload()\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=200):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "st.title(\"Llama-3 Policy Assistant\")\n",
        "prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "if st.button(\"Generate Response\"):\n",
        "    if prompt:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        response = generate_response(prompt)\n",
        "        # Split into sentences and keep first 10\n",
        "        sentences = response.strip().split(\".\")\n",
        "        short_response = \".\".join(sentences[:10]).strip() + \".\"\n",
        "        st.write(short_response)\n",
        "    else:\n",
        "        st.warning(\"Please enter a question\")\n",
        "\n",
        "  # COPY PASTE THIS BEFORE YOUR PROMPT. You are a NACCAS policy expert. Answer using only official NACCAS documentation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK1PnROdMdED",
        "outputId": "43ae2bc5-efe4-472f-ad60-a0ef4b8b32a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… MAIN EXECUTION (in separate cell)\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=\"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\")  # Replace with your token\n",
        "\n",
        "\n",
        "# Set ngrok authtoken\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")  # Replace with your token\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjLQ7TFEMpF_",
        "outputId": "37c40dd8-7352-4756-c7d2-ae229c5eba10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://9e86-34-105-83-124.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.105.83.124:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-05-21 17:37:14.544330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747849034.799655    1596 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747849034.862042    1596 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:37:15.406416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-21 17:37:21.950 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v3"
      ],
      "metadata": {
        "id": "xHuY5mbMbDfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Significantly better and coherent results. requires minor adjustment, still putting out tokens like < /s> or < /item> etc"
      ],
      "metadata": {
        "id": "L_rTOZjViAxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q unsloth peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "!pip install PyPDF2 docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaTm9wwpbFvE",
        "outputId": "fd3dc1f5-5c08-44e3-e4fb-e6d6c83a6d81"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.2.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=53c5ea0fd15e1b6c1c7bf91e8b4365dfe35dc56ed776552528388386fa5f2d7b\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: PyPDF2, docx\n",
            "Successfully installed PyPDF2-3.0.1 docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMPLE STREAMLIT GUI\n",
        "\n",
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import torch\n",
        "# from unsloth import FastLanguageModel\n",
        "# from transformers import AutoTokenizer\n",
        "# import gc\n",
        "# import re\n",
        "\n",
        "# @st.cache_resource\n",
        "# def load_model_and_tokenizer():\n",
        "#     fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "\n",
        "#     # Load the fine-tuned model and tokenizer\n",
        "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#         model_name=fine_tuned_dir,\n",
        "#         load_in_4bit=True,\n",
        "#         max_seq_length=2048,\n",
        "#         device_map=\"auto\",\n",
        "#     )\n",
        "#     model.eval()\n",
        "#     return model, tokenizer\n",
        "\n",
        "# def clean_response(response):\n",
        "#     # Remove all HTML-like tags, special tokens, and normalize whitespace\n",
        "#     response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)  # Remove tags like <item>, </item>, <s>, </s>, <span>, [INST], etc.\n",
        "#     response = re.sub(r'\\s+', ' ', response).strip()  # Normalize whitespace\n",
        "#     # Split into sentences and take the first 10 valid sentences\n",
        "#     sentences = response.split('.')\n",
        "#     cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "#     return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "# def generate_response(prompt, max_new_tokens=100):\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# # ----------------------- Format the prompt with NACCAS context -------------------------\n",
        "#     formatted_prompt = (\n",
        "#         f\"<s>[INST] You are a NACCAS policy expert. Answer the following question based strictly on official NACCAS documentation: {prompt} [/INST]\"\n",
        "#     )\n",
        "#     input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,  # Lowered for less randomness\n",
        "#             top_k=10,        # Stricter top-k for coherence\n",
        "#             top_p=0.85,      # Adjusted for more focused sampling\n",
        "#             repetition_penalty=1.3,  # Increased to reduce repetition\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "#     return clean_response(response)\n",
        "\n",
        "# st.title(\"NACCAS Policy Assistant\")\n",
        "# prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "# if st.button(\"Generate Response\"):\n",
        "#     if prompt:\n",
        "#         gc.collect()\n",
        "#         torch.cuda.empty_cache()\n",
        "#         response = generate_response(prompt)\n",
        "#         st.write(response)\n",
        "#     else:\n",
        "#         st.warning(\"Please enter a question about NACCAS policies\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kScR0pugQLn",
        "outputId": "272271d1-c5af-488b-9f15-c1481603c06f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "try:\n",
        "    import docx\n",
        "except ImportError:\n",
        "    docx = None\n",
        "import uuid\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import re\n",
        "\n",
        "# Initialize session state\n",
        "if 'chat_sessions' not in st.session_state:\n",
        "    st.session_state.chat_sessions = {}\n",
        "if 'current_chat_id' not in st.session_state:\n",
        "    st.session_state.current_chat_id = str(uuid.uuid4())\n",
        "if 'memory' not in st.session_state:\n",
        "    st.session_state.memory = []\n",
        "\n",
        "# Load persistent state from JSON\n",
        "def load_state():\n",
        "    try:\n",
        "        with open(\"chat_state.json\", \"r\") as f:\n",
        "            state = json.load(f)\n",
        "            st.session_state.chat_sessions = {\n",
        "                k: v for k, v in state.get(\"chat_sessions\", {}).items()\n",
        "            }\n",
        "            st.session_state.memory = state.get(\"memory\", [])\n",
        "            st.session_state.current_chat_id = state.get(\"current_chat_id\", str(uuid.uuid4()))\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "# Save state to JSON\n",
        "def save_state():\n",
        "    with open(\"chat_state.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            \"chat_sessions\": st.session_state.chat_sessions,\n",
        "            \"memory\": st.session_state.memory,\n",
        "            \"current_chat_id\": st.session_state.current_chat_id\n",
        "        }, f)\n",
        "\n",
        "# Load state at startup\n",
        "load_state()\n",
        "\n",
        "# Load Unsloth model and tokenizer\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=fine_tuned_dir,\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# Clean response function\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)\n",
        "    response = re.sub(r'\\s+', ' ', response).strip()\n",
        "    sentences = response.split('.')\n",
        "    cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "# Generate response function\n",
        "def generate_response(prompt, chat_history):\n",
        "    context = \"\"\n",
        "    for mem in st.session_state.memory:\n",
        "        context += f\"[MEMORY]: {mem}\\n\"\n",
        "    for msg in chat_history:\n",
        "        context += f\"{msg['role'].capitalize()}: {msg['content']}\\n\"\n",
        "    context += f\"User: {prompt}\\n\"\n",
        "\n",
        "    formatted_prompt = f\"<s>[INST] You are a NACCAS policy expert. Answer the following based strictly on official NACCAS documentation:\\n{context} [/INST]\"\n",
        "\n",
        "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.3,\n",
        "            top_k=10,\n",
        "            top_p=0.85,\n",
        "            repetition_penalty=1.3,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return clean_response(response)\n",
        "\n",
        "# Function to extract text from PDF\n",
        "def extract_pdf_text(file):\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {str(e)}\"\n",
        "\n",
        "# Function to extract text from Word document\n",
        "def extract_docx_text(file):\n",
        "    if docx is None:\n",
        "        return \"Word document support unavailable (python-docx not installed).\"\n",
        "    try:\n",
        "        doc = docx.Document(file)\n",
        "        text = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading Word document: {str(e)}\"\n",
        "\n",
        "# Function to process uploaded file\n",
        "def process_uploaded_file(uploaded_file):\n",
        "    if uploaded_file is not None:\n",
        "        if uploaded_file.name.endswith('.pdf'):\n",
        "            text = extract_pdf_text(uploaded_file)\n",
        "        elif uploaded_file.name.endswith('.docx'):\n",
        "            text = extract_docx_text(uploaded_file)\n",
        "        else:\n",
        "            text = \"Unsupported file format. Please upload a PDF or Word document.\"\n",
        "        return text\n",
        "    return None\n",
        "\n",
        "# Function to create a new chat session\n",
        "def new_chat():\n",
        "    new_chat_id = str(uuid.uuid4())\n",
        "    st.session_state.chat_sessions[new_chat_id] = []\n",
        "    st.session_state.current_chat_id = new_chat_id\n",
        "    if len(st.session_state.chat_sessions) > 5:\n",
        "        oldest_chat_id = next(iter(st.session_state.chat_sessions))\n",
        "        del st.session_state.chat_sessions[oldest_chat_id]\n",
        "    save_state()\n",
        "\n",
        "# Sidebar for chat session management\n",
        "st.sidebar.title(\"Chat Sessions\")\n",
        "if st.sidebar.button(\"New Chat\"):\n",
        "    new_chat()\n",
        "\n",
        "for chat_id in list(st.session_state.chat_sessions.keys())[:5]:\n",
        "    if st.sidebar.button(f\"Chat {chat_id[:8]}\", key=chat_id):\n",
        "        st.session_state.current_chat_id = chat_id\n",
        "        save_state()\n",
        "\n",
        "# Main chat interface\n",
        "st.title(\"NACCAS Policy Assistant\")\n",
        "\n",
        "if st.session_state.current_chat_id not in st.session_state.chat_sessions:\n",
        "    st.session_state.chat_sessions[st.session_state.current_chat_id] = []\n",
        "    save_state()\n",
        "\n",
        "# Display chat history\n",
        "st.subheader(\"Chat History\")\n",
        "for message in st.session_state.chat_sessions[st.session_state.current_chat_id]:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "\n",
        "# Display memory\n",
        "st.subheader(\"Memory\")\n",
        "if st.session_state.memory:\n",
        "    for mem in st.session_state.memory:\n",
        "        st.write(f\"- {mem}\")\n",
        "else:\n",
        "    st.write(\"No memory items yet.\")\n",
        "\n",
        "# Input form to prevent infinite reruns\n",
        "with st.form(key=\"input_form\", clear_on_submit=True):\n",
        "    col1, col2 = st.columns([1, 4])\n",
        "    with col1:\n",
        "        uploaded_file = st.file_uploader(\"Upload PDF/Word\", type=['pdf', 'docx'], label_visibility=\"collapsed\")\n",
        "    with col2:\n",
        "        user_input = st.text_input(\"Ask about NACCAS policies...\", key=\"user_input\")\n",
        "    submit_button = st.form_submit_button(\"Send\")\n",
        "\n",
        "# Process input and file on form submission\n",
        "if submit_button and (user_input or uploaded_file):\n",
        "    current_chat = st.session_state.chat_sessions[st.session_state.current_chat_id]\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if uploaded_file:\n",
        "        file_content = process_uploaded_file(uploaded_file)\n",
        "        if file_content:\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Uploaded file content:\\n{file_content}\"\n",
        "            })\n",
        "            st.session_state.memory.append(f\"Uploaded file: {uploaded_file.name}\")\n",
        "            if len(st.session_state.memory) > 5:\n",
        "                st.session_state.memory.pop(0)\n",
        "            assistant_response = generate_response(f\"User uploaded a file: {uploaded_file.name}. Content: {file_content}\", current_chat)\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": assistant_response\n",
        "            })\n",
        "            save_state()\n",
        "\n",
        "    if user_input:\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        })\n",
        "        assistant_response = generate_response(user_input, current_chat)\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_response\n",
        "        })\n",
        "        st.session_state.memory.append(f\"User asked: {user_input}\")\n",
        "        if len(st.session_state.memory) > 5:\n",
        "            st.session_state.memory.pop(0)\n",
        "        save_state()\n",
        "\n",
        "    st.rerun()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4aKE0HgGNu2",
        "outputId": "5954f0fa-a89c-4530-ccc3-6b7e72e4bae5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# âœ… MAIN EXECUTION\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face (replace with your token)\n",
        "login(token=\"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\")\n",
        "\n",
        "# Set ngrok authtoken (replace with your token)\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4Mm2ADgOjv",
        "outputId": "c57f20f0-df62-4993-b889-42784bdf4582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://ea69-34-125-99-167.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.99.167:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}