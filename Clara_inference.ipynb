{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wcttmyDOLa6z",
        "CYgm6hWPTW_T",
        "HhP32UuhMd7O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8880c5bdb0b94406baa6e8c11c2b4b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d46e2dd0f90460aaa697121d99e0bf8",
              "IPY_MODEL_25f17d9ed7f647a2a509127d8ad4613f",
              "IPY_MODEL_c17bfdc5801c4fd7a4b1dfae84079651"
            ],
            "layout": "IPY_MODEL_b068f9f0d26d4c3b90db39c62e33b93d"
          }
        },
        "8d46e2dd0f90460aaa697121d99e0bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6fcc9514be4c27bdac719631feb87f",
            "placeholder": "​",
            "style": "IPY_MODEL_c7cc96d21d5e4e6697ed42a46c705ab2",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "25f17d9ed7f647a2a509127d8ad4613f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665636d48b8a41e3842c3da4ad2c516c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f164bdfa58e64095a60e8af4fc45465d",
            "value": 2
          }
        },
        "c17bfdc5801c4fd7a4b1dfae84079651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad56c5f41315422988b36848c624ace7",
            "placeholder": "​",
            "style": "IPY_MODEL_5308eb24baa443e2a4bf9a5f6110acb9",
            "value": " 2/2 [00:30&lt;00:00, 13.98s/it]"
          }
        },
        "b068f9f0d26d4c3b90db39c62e33b93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6fcc9514be4c27bdac719631feb87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7cc96d21d5e4e6697ed42a46c705ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665636d48b8a41e3842c3da4ad2c516c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f164bdfa58e64095a60e8af4fc45465d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad56c5f41315422988b36848c624ace7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5308eb24baa443e2a4bf9a5f6110acb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c7de962b8bb42afb2b38a4501216ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_678958cec02c432abbc33bb4b31b3ecf",
              "IPY_MODEL_0fe11ee33bed4af7be9a79ae6d0c72c0",
              "IPY_MODEL_4718dcdcd78c4f05812c56f4bde3ff15"
            ],
            "layout": "IPY_MODEL_8176f495ac874e34b9e877e315045a7b"
          }
        },
        "678958cec02c432abbc33bb4b31b3ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22138cc16e334027a55f77ff83903e4d",
            "placeholder": "​",
            "style": "IPY_MODEL_15d31e8d6b064248be42e071c3408f37",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "0fe11ee33bed4af7be9a79ae6d0c72c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0c6be6e0484b0f8ee41d5f14fc7333",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_785d4fe3d2644ae795ee5e4af45cea2e",
            "value": 2
          }
        },
        "4718dcdcd78c4f05812c56f4bde3ff15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f958024ab52d4d3aacd4ba40b2ec1cb3",
            "placeholder": "​",
            "style": "IPY_MODEL_59877dd24f8d42afaa3efed1a8e54a22",
            "value": " 2/2 [00:19&lt;00:00,  9.66s/it]"
          }
        },
        "8176f495ac874e34b9e877e315045a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22138cc16e334027a55f77ff83903e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d31e8d6b064248be42e071c3408f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b0c6be6e0484b0f8ee41d5f14fc7333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "785d4fe3d2644ae795ee5e4af45cea2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f958024ab52d4d3aacd4ba40b2ec1cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59877dd24f8d42afaa3efed1a8e54a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#V2 inference"
      ],
      "metadata": {
        "id": "wcttmyDOLa6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes pyngrok\n",
        "\n",
        "# =========================\n",
        "# ✅ IMPORTS\n",
        "# =========================\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login\n",
        "import zipfile\n",
        "import os\n",
        "import gc\n",
        "# =========================\n",
        "# ✅ AUTHENTICATION\n",
        "# =========================\n",
        "login(token=\"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\")  # Replace securely in production\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQWe8S0CLaoF",
        "outputId": "b629c6da-1bf4-41c8-a60d-d1fa5b1aa6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ FILE EXTRACTION\n",
        "# =========================\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Extraction complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IV-CmLeLrQ-",
        "outputId": "3a6b3038-44eb-40f8-8885-7f110d6e9e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# ✅ PATHS & CONFIGS\n",
        "# =========================\n",
        "base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# ✅ LOAD MODEL & TOKENIZER\n",
        "# =========================\n",
        "def load_model_and_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "# =========================\n",
        "# ✅ INFERENCE FUNCTION\n",
        "# =========================\n",
        "def generate_response(prompt, max_new_tokens=80):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "xradrsRjLxoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ EXAMPLE RUN\n",
        "# =========================\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "prompt = \"You are a NACCAS policy expert. Answer using only official NACCAS documentation. can a student with ged get admission in NACCAS accredited institute?\"\n",
        "response = generate_response(prompt)\n",
        "# Split into sentences and keep first 3\n",
        "sentences = response.strip().split(\".\")\n",
        "short_response = \".\".join(sentences[:3]).strip() + \".\"\n",
        "print(\"🧠 Model response:\\n\", short_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "8880c5bdb0b94406baa6e8c11c2b4b05",
            "8d46e2dd0f90460aaa697121d99e0bf8",
            "25f17d9ed7f647a2a509127d8ad4613f",
            "c17bfdc5801c4fd7a4b1dfae84079651",
            "b068f9f0d26d4c3b90db39c62e33b93d",
            "7b6fcc9514be4c27bdac719631feb87f",
            "c7cc96d21d5e4e6697ed42a46c705ab2",
            "665636d48b8a41e3842c3da4ad2c516c",
            "f164bdfa58e64095a60e8af4fc45465d",
            "ad56c5f41315422988b36848c624ace7",
            "5308eb24baa443e2a4bf9a5f6110acb9"
          ]
        },
        "id": "mg25KBfqL0ak",
        "outputId": "3b5bf32a-f3c2-482c-b131-7b5675e34a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8880c5bdb0b94406baa6e8c11c2b4b05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🧠 Model response:\n",
            " A copy of the student?s high school transcript or certificate must be maintained\n",
            "on file at the institution and a copy is to be sent to the student?s home address.\n",
            "33. If an institution becomes subject to one or more Show/Cause Orders, the school shall\n",
            "send a copy to all enrolled students.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V2.5 inference (not useful right now)"
      ],
      "metadata": {
        "id": "CYgm6hWPTW_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19QjBrX77-ET",
        "outputId": "d81658bf-64ff-4ab5-9f8a-fce8c07e10d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# # =========================\n",
        "# # ✅ SETUP & DEPENDENCIES\n",
        "# # =========================\n",
        "# !pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "\n",
        "# # =========================\n",
        "# # ✅ IMPORTS\n",
        "# # =========================\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from peft import PeftModel\n",
        "# from pyngrok import ngrok\n",
        "# import zipfile\n",
        "# import os\n",
        "# from huggingface_hub import login\n",
        "# # %%writefile app.py\n",
        "# # import streamlit as st\n",
        "# # !ngrok config add-authtoken\n",
        "\n",
        "# # =========================\n",
        "# # ✅ AUTHENTICATION\n",
        "# # =========================\n",
        "# # Login to Hugging Face (you might want to handle this differently for security)\n",
        "# login(token=\"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\")  # Replace with your token or use environment variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # ✅ FILE EXTRACTION\n",
        "# # =========================\n",
        "# zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "# extract_path = \"/content\"\n",
        "\n",
        "# # Extract the zip\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(\"✅ Extraction complete!\")\n",
        "\n",
        "# # =========================\n",
        "# # ✅ PATHS & CONFIGS\n",
        "# # =========================\n",
        "# base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "# fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "# )"
      ],
      "metadata": {
        "id": "jyyQNb1KL_KS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3653b01a-e1dd-4d7d-ccf0-000e9123b1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # ✅ INFERENCE FUNCTION\n",
        "# # =========================\n",
        "\n",
        "# def generate_response(user_prompt, system_prompt, max_new_tokens=150):\n",
        "#     # Load model and tokenizer\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "#     # Create properly formatted Llama 3 prompt\n",
        "#     prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "#     Cutting Knowledge Date: December 2023\n",
        "#     Today Date: 23 July 2024\n",
        "#     {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "#     {user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "#     # Tokenize and move to model's device\n",
        "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     # Generate response\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,\n",
        "#             top_k=20,\n",
        "#             top_p=0.9,\n",
        "#             repetition_penalty=1.2,\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     # Decode only the assistant's response (skip the input prompt)\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "#     return response.strip()"
      ],
      "metadata": {
        "id": "iEvlDqdX9lrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # =========================\n",
        "# # # ✅ TEST PROMPT\n",
        "# # # =========================\n",
        "# # prompt = \"What must a NACCAS-accredited institute include in its school catalog regarding its Ability to Benefit policy?\"\n",
        "# system_prompt = \"You are a NACCAS policy expert. Answer only using official documents.\"\n",
        "# user_prompt = \"What must NACCAS accredited institute include in its school catalog regarding its ability to benifit policy?.\"\n",
        "\n",
        "# response = generate_response(user_prompt,system_prompt)\n",
        "# print(\"🧠 Model response:\\n\", response)"
      ],
      "metadata": {
        "id": "s-_bK6M892zK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "7c7de962b8bb42afb2b38a4501216ae6",
            "678958cec02c432abbc33bb4b31b3ecf",
            "0fe11ee33bed4af7be9a79ae6d0c72c0",
            "4718dcdcd78c4f05812c56f4bde3ff15",
            "8176f495ac874e34b9e877e315045a7b",
            "22138cc16e334027a55f77ff83903e4d",
            "15d31e8d6b064248be42e071c3408f37",
            "2b0c6be6e0484b0f8ee41d5f14fc7333",
            "785d4fe3d2644ae795ee5e4af45cea2e",
            "f958024ab52d4d3aacd4ba40b2ec1cb3",
            "59877dd24f8d42afaa3efed1a8e54a22"
          ]
        },
        "outputId": "6ac92881-6770-4dc6-be95-0ed20be859ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c7de962b8bb42afb2b38a4501216ae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-3.2-3B', revision=None, inference_mode=True, r=32, target_modules={'up_proj', 'gate_proj', 'down_proj', 'v_proj', 'k_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
            "🧠 Model response:\n",
            " A copy of the current accreditation agreement between the institution and NACCCS is included in the catalog.圭圭ニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニニ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v2 + streamlit and ngrok both on colab"
      ],
      "metadata": {
        "id": "HhP32UuhMd7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ FILE EXTRACTION\n",
        "# =========================\n",
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "# Extract the zip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"✅ Extraction complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0pEr-kbMrG0",
        "outputId": "8cb17e70-fbf1-4009-a296-fafc8d3e8928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe05jDuSMjhz",
        "outputId": "2a6d71ce-5de2-41e2-cd46-85e08aab7d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ CREATE STREAMLIT APP FILE (in separate cell)\n",
        "# =========================\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "    fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(fine_tuned_dir, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model = model.merge_and_unload()\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=200):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "st.title(\"Llama-3 Policy Assistant\")\n",
        "prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "if st.button(\"Generate Response\"):\n",
        "    if prompt:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        response = generate_response(prompt)\n",
        "        # Split into sentences and keep first 10\n",
        "        sentences = response.strip().split(\".\")\n",
        "        short_response = \".\".join(sentences[:10]).strip() + \".\"\n",
        "        st.write(short_response)\n",
        "    else:\n",
        "        st.warning(\"Please enter a question\")\n",
        "\n",
        "  # COPY PASTE THIS BEFORE YOUR PROMPT. You are a NACCAS policy expert. Answer using only official NACCAS documentation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK1PnROdMdED",
        "outputId": "43ae2bc5-efe4-472f-ad60-a0ef4b8b32a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ MAIN EXECUTION (in separate cell)\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=\"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\")  # Replace with your token\n",
        "\n",
        "\n",
        "# Set ngrok authtoken\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")  # Replace with your token\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjLQ7TFEMpF_",
        "outputId": "37c40dd8-7352-4756-c7d2-ae229c5eba10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://9e86-34-105-83-124.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.105.83.124:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-05-21 17:37:14.544330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747849034.799655    1596 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747849034.862042    1596 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:37:15.406416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-21 17:37:21.950 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v3"
      ],
      "metadata": {
        "id": "xHuY5mbMbDfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Significantly better and coherent results. requires minor adjustment, still putting out tokens like < /s> or < /item> etc"
      ],
      "metadata": {
        "id": "L_rTOZjViAxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q unsloth peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "!pip install PyPDF2 docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaTm9wwpbFvE",
        "outputId": "ee165d2d-d77b-4c1e-a050-643faea462e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m854.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.2.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=7076cd51b5570e68b4ee08e9ded08a4d211f25c8d2a42c83ccb32952dd5f2960\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: PyPDF2, docx\n",
            "Successfully installed PyPDF2-3.0.1 docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMPLE STREAMLIT GUI\n",
        "\n",
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import torch\n",
        "# from unsloth import FastLanguageModel\n",
        "# from transformers import AutoTokenizer\n",
        "# import gc\n",
        "# import re\n",
        "\n",
        "# @st.cache_resource\n",
        "# def load_model_and_tokenizer():\n",
        "#     fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "\n",
        "#     # Load the fine-tuned model and tokenizer\n",
        "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#         model_name=fine_tuned_dir,\n",
        "#         load_in_4bit=True,\n",
        "#         max_seq_length=2048,\n",
        "#         device_map=\"auto\",\n",
        "#     )\n",
        "#     model.eval()\n",
        "#     return model, tokenizer\n",
        "\n",
        "# def clean_response(response):\n",
        "#     # Remove all HTML-like tags, special tokens, and normalize whitespace\n",
        "#     response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)  # Remove tags like <item>, </item>, <s>, </s>, <span>, [INST], etc.\n",
        "#     response = re.sub(r'\\s+', ' ', response).strip()  # Normalize whitespace\n",
        "#     # Split into sentences and take the first 10 valid sentences\n",
        "#     sentences = response.split('.')\n",
        "#     cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "#     return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "# def generate_response(prompt, max_new_tokens=100):\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# # ----------------------- Format the prompt with NACCAS context -------------------------\n",
        "#     formatted_prompt = (\n",
        "#         f\"<s>[INST] You are a NACCAS policy expert. Answer the following question based strictly on official NACCAS documentation: {prompt} [/INST]\"\n",
        "#     )\n",
        "#     input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,  # Lowered for less randomness\n",
        "#             top_k=10,        # Stricter top-k for coherence\n",
        "#             top_p=0.85,      # Adjusted for more focused sampling\n",
        "#             repetition_penalty=1.3,  # Increased to reduce repetition\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "#     return clean_response(response)\n",
        "\n",
        "# st.title(\"NACCAS Policy Assistant\")\n",
        "# prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "# if st.button(\"Generate Response\"):\n",
        "#     if prompt:\n",
        "#         gc.collect()\n",
        "#         torch.cuda.empty_cache()\n",
        "#         response = generate_response(prompt)\n",
        "#         st.write(response)\n",
        "#     else:\n",
        "#         st.warning(\"Please enter a question about NACCAS policies\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kScR0pugQLn",
        "outputId": "272271d1-c5af-488b-9f15-c1481603c06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "try:\n",
        "    import docx\n",
        "except ImportError:\n",
        "    docx = None\n",
        "import uuid\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import re\n",
        "\n",
        "# Initialize session state\n",
        "if 'chat_sessions' not in st.session_state:\n",
        "    st.session_state.chat_sessions = {}\n",
        "if 'current_chat_id' not in st.session_state:\n",
        "    st.session_state.current_chat_id = str(uuid.uuid4())\n",
        "if 'memory' not in st.session_state:\n",
        "    st.session_state.memory = []\n",
        "if 'chat_names' not in st.session_state:\n",
        "    st.session_state.chat_names = {}\n",
        "\n",
        "# Load persistent state from JSON\n",
        "def load_state():\n",
        "    try:\n",
        "        with open(\"chat_state.json\", \"r\") as f:\n",
        "            state = json.load(f)\n",
        "            st.session_state.chat_sessions = state.get(\"chat_sessions\", {})\n",
        "            st.session_state.memory = state.get(\"memory\", [])\n",
        "            st.session_state.current_chat_id = state.get(\"current_chat_id\", str(uuid.uuid4()))\n",
        "            st.session_state.chat_names = state.get(\"chat_names\", {})\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "def save_state():\n",
        "    with open(\"chat_state.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            \"chat_sessions\": st.session_state.chat_sessions,\n",
        "            \"memory\": st.session_state.memory,\n",
        "            \"current_chat_id\": st.session_state.current_chat_id,\n",
        "            \"chat_names\": st.session_state.chat_names\n",
        "        }, f)\n",
        "\n",
        "load_state()\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=fine_tuned_dir,\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)\n",
        "    response = re.sub(r'\\s+', ' ', response).strip()\n",
        "    sentences = response.split('.')\n",
        "    cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "def generate_response(prompt, chat_history=None):\n",
        "    formatted_prompt = f\"<s>[INST] You are a NACCAS policy expert. Answer the following based strictly on official NACCAS documentation:\\nUser: {prompt} [/INST]\"\n",
        "\n",
        "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.3,\n",
        "            top_k=10,\n",
        "            top_p=0.85,\n",
        "            repetition_penalty=1.3,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return clean_response(response)\n",
        "\n",
        "def extract_pdf_text(file):\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {str(e)}\"\n",
        "\n",
        "def extract_docx_text(file):\n",
        "    if docx is None:\n",
        "        return \"Word document support unavailable (python-docx not installed).\"\n",
        "    try:\n",
        "        doc = docx.Document(file)\n",
        "        text = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading Word document: {str(e)}\"\n",
        "\n",
        "def process_uploaded_file(uploaded_file):\n",
        "    if uploaded_file is not None:\n",
        "        if uploaded_file.name.endswith('.pdf'):\n",
        "            text = extract_pdf_text(uploaded_file)\n",
        "        elif uploaded_file.name.endswith('.docx'):\n",
        "            text = extract_docx_text(uploaded_file)\n",
        "        else:\n",
        "            text = \"Unsupported file format. Please upload a PDF or Word document.\"\n",
        "        return text\n",
        "    return None\n",
        "\n",
        "def new_chat():\n",
        "    new_chat_id = str(uuid.uuid4())\n",
        "    st.session_state.chat_sessions[new_chat_id] = []\n",
        "    st.session_state.current_chat_id = new_chat_id\n",
        "    if len(st.session_state.chat_sessions) > 5:\n",
        "        oldest_chat_id = next(iter(st.session_state.chat_sessions))\n",
        "        del st.session_state.chat_sessions[oldest_chat_id]\n",
        "        if oldest_chat_id in st.session_state.chat_names:\n",
        "            del st.session_state.chat_names[oldest_chat_id]\n",
        "    save_state()\n",
        "\n",
        "def update_chat_name(chat_id, message):\n",
        "    if chat_id not in st.session_state.chat_names and message[\"role\"] == \"user\":\n",
        "        words = message[\"content\"].split()[:5]\n",
        "        st.session_state.chat_names[chat_id] = \" \".join(words) + \" ...\"\n",
        "        save_state()\n",
        "\n",
        "# ---------- UI ----------\n",
        "\n",
        "# CSS Styles\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        ".sidebar .sidebar-content {\n",
        "    width: 200px !important;\n",
        "}\n",
        ".css-1vq4p4l.e1f1d6gn2 {\n",
        "    padding-bottom: 90px !important;\n",
        "}\n",
        "div[data-testid=\"stForm\"] input[type=\"text\"] {\n",
        "    width: 100% !important;\n",
        "    max-width: 500px !important;\n",
        "}\n",
        "\n",
        "input[type=\"text\"] {\n",
        "    font-size: 0.85rem !important;\n",
        "    height: 2.2rem !important;\n",
        "    padding: 0.3rem 0.5rem !important;\n",
        "    width: 100% !important;\n",
        "}\n",
        "button[kind=\"primary\"] {\n",
        "    font-size: 0.85rem !important;\n",
        "    padding: 0.3rem 1rem !important;\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"Chat Sessions\")\n",
        "st.sidebar.button(\"New Chat\", on_click=new_chat)\n",
        "\n",
        "for chat_id in list(st.session_state.chat_sessions.keys())[:5]:\n",
        "    chat_name = st.session_state.chat_names.get(chat_id, f\"Chat {chat_id[:8]}\")\n",
        "    if st.sidebar.button(chat_name, key=chat_id):\n",
        "        st.session_state.current_chat_id = chat_id\n",
        "        save_state()\n",
        "\n",
        "# Main Title\n",
        "st.title(\"NACCAS Policy Assistant\")\n",
        "\n",
        "# Chat history\n",
        "if st.session_state.current_chat_id not in st.session_state.chat_sessions:\n",
        "    st.session_state.chat_sessions[st.session_state.current_chat_id] = []\n",
        "    save_state()\n",
        "\n",
        "st.subheader(\"Chat History\")\n",
        "for message in st.session_state.chat_sessions[st.session_state.current_chat_id]:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "    if message[\"role\"] == \"user\" and st.session_state.current_chat_id not in st.session_state.chat_names:\n",
        "        update_chat_name(st.session_state.current_chat_id, message)\n",
        "\n",
        "# Fixed Input Bar Form\n",
        "with st.form(key=\"input_form\", clear_on_submit=True):\n",
        "    upload_col, input_col, button_col = st.columns([1, 3, 1])\n",
        "    with upload_col:\n",
        "        uploaded_file = st.file_uploader(\"\", type=['pdf', 'docx'], label_visibility=\"collapsed\")\n",
        "    with input_col:\n",
        "        user_input = st.text_input(\"\", placeholder=\"Ask about NACCAS policies...\", key=\"user_input\")\n",
        "    with button_col:\n",
        "        submit_button = st.form_submit_button(\"Send\")\n",
        "\n",
        "if submit_button and (user_input or uploaded_file):\n",
        "    current_chat = st.session_state.chat_sessions[st.session_state.current_chat_id]\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if uploaded_file:\n",
        "        file_content = process_uploaded_file(uploaded_file)\n",
        "        if file_content:\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Uploaded file content:\\n{file_content}\"\n",
        "            })\n",
        "            update_chat_name(st.session_state.current_chat_id, {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Uploaded file content:\\n{file_content}\"\n",
        "            })\n",
        "            assistant_response = generate_response(f\"User uploaded a file: {uploaded_file.name}. Content: {file_content}\", current_chat)\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": assistant_response\n",
        "            })\n",
        "            save_state()\n",
        "\n",
        "    if user_input:\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        })\n",
        "        update_chat_name(st.session_state.current_chat_id, {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        })\n",
        "        assistant_response = generate_response(user_input, current_chat)\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_response\n",
        "        })\n",
        "        save_state()\n",
        "\n",
        "    st.rerun()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4aKE0HgGNu2",
        "outputId": "6b7ee632-6f3f-442e-bba7-bd1906a8696d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# ✅ MAIN EXECUTION\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face (replace with your token)\n",
        "login(token=\"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\")\n",
        "\n",
        "# Set ngrok authtoken (replace with your token)\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4Mm2ADgOjv",
        "outputId": "49e4249c-e89a-4e27-aa3e-b6cbe5f83a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://ff52-34-168-209-159.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.168.209.159:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-05-27 17:25:58.745 Uncaught exception GET /_stcore/stream (127.0.0.1)\n",
            "HTTPServerRequest(protocol='http', host='ff52-34-168-209-159.ngrok-free.app', method='GET', uri='/_stcore/stream', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/websocket.py\", line 938, in _accept_connection\n",
            "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/browser_websocket_handler.py\", line 177, in open\n",
            "    self._session_id = self._runtime.connect_session(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 384, in connect_session\n",
            "    session_id = self._session_mgr.connect_session(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/websocket_session_manager.py\", line 99, in connect_session\n",
            "    session = AppSession(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 158, in __init__\n",
            "    self.register_file_watchers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 195, in register_file_watchers\n",
            "    self._local_sources_watcher = LocalSourcesWatcher(self._pages_manager)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 65, in __init__\n",
            "    self.update_watched_pages()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 77, in update_watched_pages\n",
            "    self._register_watcher(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 136, in _register_watcher\n",
            "    watcher=PathWatcher(filepath, self.on_file_changed),\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 107, in __init__\n",
            "    path_watcher.watch_path(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 185, in watch_path\n",
            "    folder_handler.watch = self._observer.schedule(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/api.py\", line 312, in schedule\n",
            "    emitter.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/utils/__init__.py\", line 75, in start\n",
            "    self.on_thread_start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify.py\", line 119, in on_thread_start\n",
            "    self._inotify = InotifyBuffer(path, recursive=self.watch.is_recursive, event_mask=event_mask)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_buffer.py\", line 30, in __init__\n",
            "    self._inotify = Inotify(path, recursive=recursive, event_mask=event_mask)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 185, in __init__\n",
            "    self._add_dir_watch(path, event_mask, recursive=recursive)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 411, in _add_dir_watch\n",
            "    self._add_watch(full_path, mask)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 424, in _add_watch\n",
            "    Inotify._raise_error()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 441, in _raise_error\n",
            "    raise OSError(err, os.strerror(err))\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Exception ignored in: <function AppSession.__del__ at 0x78bfe8e7bc40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 178, in __del__\n",
            "    self.shutdown()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 255, in shutdown\n",
            "    self.request_script_stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 428, in request_script_stop\n",
            "    if self._scriptrunner is not None:\n",
            "       ^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'AppSession' object has no attribute '_scriptrunner'\n",
            "🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "2025-05-27 17:26:06.016985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748366766.040699    5310 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748366766.047921    5310 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-27 17:26:06.070884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "🦥 Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth 2025.5.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
            "2025-05-27 17:26:29.004 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:26:29.005 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:26:30.060 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-05-27 17:26:52.140 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:26:52.141 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "2025-05-27 17:27:04.315 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:04.316 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:05.424 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-05-27 17:27:53.992 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:53.993 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:58.328 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:58.329 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-05-27T17:28:31+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-7d785337-271d-4259-bd43-c41aa4b8fca2 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-05-27T17:28:31+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-7d785337-271d-4259-bd43-c41aa4b8fca2 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in atexit callback: <function shutdown_compile_workers at 0x78be4cd08e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/async_compile.py\", line 113, in shutdown_compile_workers\n",
            "    pool.shutdown()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 237, in shutdown\n",
            "    _send_msg(self.write_pipe, -1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 51, in _send_msg\n",
            "    write_pipe.flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/server.py\", line 469, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 324, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 807, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 520, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ]
    }
  ]
}