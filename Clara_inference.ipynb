{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wcttmyDOLa6z",
        "CYgm6hWPTW_T",
        "HhP32UuhMd7O"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8880c5bdb0b94406baa6e8c11c2b4b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d46e2dd0f90460aaa697121d99e0bf8",
              "IPY_MODEL_25f17d9ed7f647a2a509127d8ad4613f",
              "IPY_MODEL_c17bfdc5801c4fd7a4b1dfae84079651"
            ],
            "layout": "IPY_MODEL_b068f9f0d26d4c3b90db39c62e33b93d"
          }
        },
        "8d46e2dd0f90460aaa697121d99e0bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6fcc9514be4c27bdac719631feb87f",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_c7cc96d21d5e4e6697ed42a46c705ab2",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "25f17d9ed7f647a2a509127d8ad4613f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_665636d48b8a41e3842c3da4ad2c516c",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f164bdfa58e64095a60e8af4fc45465d",
            "value": 2
          }
        },
        "c17bfdc5801c4fd7a4b1dfae84079651": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad56c5f41315422988b36848c624ace7",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_5308eb24baa443e2a4bf9a5f6110acb9",
            "value": "â€‡2/2â€‡[00:30&lt;00:00,â€‡13.98s/it]"
          }
        },
        "b068f9f0d26d4c3b90db39c62e33b93d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b6fcc9514be4c27bdac719631feb87f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7cc96d21d5e4e6697ed42a46c705ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "665636d48b8a41e3842c3da4ad2c516c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f164bdfa58e64095a60e8af4fc45465d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ad56c5f41315422988b36848c624ace7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5308eb24baa443e2a4bf9a5f6110acb9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c7de962b8bb42afb2b38a4501216ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_678958cec02c432abbc33bb4b31b3ecf",
              "IPY_MODEL_0fe11ee33bed4af7be9a79ae6d0c72c0",
              "IPY_MODEL_4718dcdcd78c4f05812c56f4bde3ff15"
            ],
            "layout": "IPY_MODEL_8176f495ac874e34b9e877e315045a7b"
          }
        },
        "678958cec02c432abbc33bb4b31b3ecf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_22138cc16e334027a55f77ff83903e4d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_15d31e8d6b064248be42e071c3408f37",
            "value": "Loadingâ€‡checkpointâ€‡shards:â€‡100%"
          }
        },
        "0fe11ee33bed4af7be9a79ae6d0c72c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b0c6be6e0484b0f8ee41d5f14fc7333",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_785d4fe3d2644ae795ee5e4af45cea2e",
            "value": 2
          }
        },
        "4718dcdcd78c4f05812c56f4bde3ff15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f958024ab52d4d3aacd4ba40b2ec1cb3",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_59877dd24f8d42afaa3efed1a8e54a22",
            "value": "â€‡2/2â€‡[00:19&lt;00:00,â€‡â€‡9.66s/it]"
          }
        },
        "8176f495ac874e34b9e877e315045a7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22138cc16e334027a55f77ff83903e4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "15d31e8d6b064248be42e071c3408f37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b0c6be6e0484b0f8ee41d5f14fc7333": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "785d4fe3d2644ae795ee5e4af45cea2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f958024ab52d4d3aacd4ba40b2ec1cb3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59877dd24f8d42afaa3efed1a8e54a22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#V2 inference"
      ],
      "metadata": {
        "id": "wcttmyDOLa6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes pyngrok\n",
        "\n",
        "# =========================\n",
        "# âœ… IMPORTS\n",
        "# =========================\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "from huggingface_hub import login\n",
        "import zipfile\n",
        "import os\n",
        "import gc\n",
        "# =========================\n",
        "# âœ… AUTHENTICATION\n",
        "# =========================\n",
        "login(token=\"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\")  # Replace securely in production\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JQWe8S0CLaoF",
        "outputId": "b629c6da-1bf4-41c8-a60d-d1fa5b1aa6ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m107.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… FILE EXTRACTION\n",
        "# =========================\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"âœ… Extraction complete!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9IV-CmLeLrQ-",
        "outputId": "3a6b3038-44eb-40f8-8885-7f110d6e9e69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# =========================\n",
        "# âœ… PATHS & CONFIGS\n",
        "# =========================\n",
        "base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# =========================\n",
        "# âœ… LOAD MODEL & TOKENIZER\n",
        "# =========================\n",
        "def load_model_and_tokenizer():\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\"\n",
        "    )\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "# =========================\n",
        "# âœ… INFERENCE FUNCTION\n",
        "# =========================\n",
        "def generate_response(prompt, max_new_tokens=80):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n"
      ],
      "metadata": {
        "id": "xradrsRjLxoJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… EXAMPLE RUN\n",
        "# =========================\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "prompt = \"You are a NACCAS policy expert. Answer using only official NACCAS documentation. can a student with ged get admission in NACCAS accredited institute?\"\n",
        "response = generate_response(prompt)\n",
        "# Split into sentences and keep first 3\n",
        "sentences = response.strip().split(\".\")\n",
        "short_response = \".\".join(sentences[:3]).strip() + \".\"\n",
        "print(\"ğŸ§  Model response:\\n\", short_response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "8880c5bdb0b94406baa6e8c11c2b4b05",
            "8d46e2dd0f90460aaa697121d99e0bf8",
            "25f17d9ed7f647a2a509127d8ad4613f",
            "c17bfdc5801c4fd7a4b1dfae84079651",
            "b068f9f0d26d4c3b90db39c62e33b93d",
            "7b6fcc9514be4c27bdac719631feb87f",
            "c7cc96d21d5e4e6697ed42a46c705ab2",
            "665636d48b8a41e3842c3da4ad2c516c",
            "f164bdfa58e64095a60e8af4fc45465d",
            "ad56c5f41315422988b36848c624ace7",
            "5308eb24baa443e2a4bf9a5f6110acb9"
          ]
        },
        "id": "mg25KBfqL0ak",
        "outputId": "3b5bf32a-f3c2-482c-b131-7b5675e34a19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8880c5bdb0b94406baa6e8c11c2b4b05"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  Model response:\n",
            " A copy of the student?s high school transcript or certificate must be maintained\n",
            "on file at the institution and a copy is to be sent to the student?s home address.\n",
            "33. If an institution becomes subject to one or more Show/Cause Orders, the school shall\n",
            "send a copy to all enrolled students.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#V2.5 inference (not useful right now)"
      ],
      "metadata": {
        "id": "CYgm6hWPTW_T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19QjBrX77-ET",
        "outputId": "d81658bf-64ff-4ab5-9f8a-fce8c07e10d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m63.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m112.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m90.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# # =========================\n",
        "# # âœ… SETUP & DEPENDENCIES\n",
        "# # =========================\n",
        "# !pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "\n",
        "# # =========================\n",
        "# # âœ… IMPORTS\n",
        "# # =========================\n",
        "# import torch\n",
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "# from peft import PeftModel\n",
        "# from pyngrok import ngrok\n",
        "# import zipfile\n",
        "# import os\n",
        "# from huggingface_hub import login\n",
        "# # %%writefile app.py\n",
        "# # import streamlit as st\n",
        "# # !ngrok config add-authtoken\n",
        "\n",
        "# # =========================\n",
        "# # âœ… AUTHENTICATION\n",
        "# # =========================\n",
        "# # Login to Hugging Face (you might want to handle this differently for security)\n",
        "# login(token=\"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\")  # Replace with your token or use environment variables"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # âœ… FILE EXTRACTION\n",
        "# # =========================\n",
        "# zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "# extract_path = \"/content\"\n",
        "\n",
        "# # Extract the zip\n",
        "# with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall(extract_path)\n",
        "\n",
        "# print(\"âœ… Extraction complete!\")\n",
        "\n",
        "# # =========================\n",
        "# # âœ… PATHS & CONFIGS\n",
        "# # =========================\n",
        "# base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "# fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "# bnb_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "#     bnb_4bit_quant_type=\"nf4\"\n",
        "# )"
      ],
      "metadata": {
        "id": "jyyQNb1KL_KS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3653b01a-e1dd-4d7d-ccf0-000e9123b1bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # =========================\n",
        "# # âœ… INFERENCE FUNCTION\n",
        "# # =========================\n",
        "\n",
        "# def generate_response(user_prompt, system_prompt, max_new_tokens=150):\n",
        "#     # Load model and tokenizer\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "#     # Create properly formatted Llama 3 prompt\n",
        "#     prompt = f\"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "#     Cutting Knowledge Date: December 2023\n",
        "#     Today Date: 23 July 2024\n",
        "#     {system_prompt}<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "#     {user_prompt}<|eot_id|><|start_header_id|>assistant<|end_header_id|>\"\"\"\n",
        "\n",
        "#     # Tokenize and move to model's device\n",
        "#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     # Generate response\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,\n",
        "#             top_k=20,\n",
        "#             top_p=0.9,\n",
        "#             repetition_penalty=1.2,\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     # Decode only the assistant's response (skip the input prompt)\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "\n",
        "#     return response.strip()"
      ],
      "metadata": {
        "id": "iEvlDqdX9lrn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# # # =========================\n",
        "# # # âœ… TEST PROMPT\n",
        "# # # =========================\n",
        "# # prompt = \"What must a NACCAS-accredited institute include in its school catalog regarding its Ability to Benefit policy?\"\n",
        "# system_prompt = \"You are a NACCAS policy expert. Answer only using official documents.\"\n",
        "# user_prompt = \"What must NACCAS accredited institute include in its school catalog regarding its ability to benifit policy?.\"\n",
        "\n",
        "# response = generate_response(user_prompt,system_prompt)\n",
        "# print(\"ğŸ§  Model response:\\n\", response)"
      ],
      "metadata": {
        "id": "s-_bK6M892zK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156,
          "referenced_widgets": [
            "7c7de962b8bb42afb2b38a4501216ae6",
            "678958cec02c432abbc33bb4b31b3ecf",
            "0fe11ee33bed4af7be9a79ae6d0c72c0",
            "4718dcdcd78c4f05812c56f4bde3ff15",
            "8176f495ac874e34b9e877e315045a7b",
            "22138cc16e334027a55f77ff83903e4d",
            "15d31e8d6b064248be42e071c3408f37",
            "2b0c6be6e0484b0f8ee41d5f14fc7333",
            "785d4fe3d2644ae795ee5e4af45cea2e",
            "f958024ab52d4d3aacd4ba40b2ec1cb3",
            "59877dd24f8d42afaa3efed1a8e54a22"
          ]
        },
        "outputId": "6ac92881-6770-4dc6-be95-0ed20be859ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c7de962b8bb42afb2b38a4501216ae6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/bnb.py:351: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'default': LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='meta-llama/Llama-3.2-3B', revision=None, inference_mode=True, r=32, target_modules={'up_proj', 'gate_proj', 'down_proj', 'v_proj', 'k_proj', 'o_proj', 'q_proj'}, exclude_modules=None, lora_alpha=16, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False)}\n",
            "ğŸ§  Model response:\n",
            " A copy of the current accreditation agreement between the institution and NACCCS is included in the catalog.åœ­åœ­ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹ãƒ‹\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v2 + streamlit and ngrok both on colab"
      ],
      "metadata": {
        "id": "HhP32UuhMd7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… FILE EXTRACTION\n",
        "# =========================\n",
        "import zipfile\n",
        "zip_path = \"/content/drive/MyDrive/llama3_policy_finetune_v2.5.zip\"\n",
        "extract_path = \"/content\"\n",
        "\n",
        "# Extract the zip\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"âœ… Extraction complete!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c0pEr-kbMrG0",
        "outputId": "8cb17e70-fbf1-4009-a296-fafc8d3e8928"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q peft transformers accelerate bitsandbytes streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xe05jDuSMjhz",
        "outputId": "2a6d71ce-5de2-41e2-cd46-85e08aab7d0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m94.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m71.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… CREATE STREAMLIT APP FILE (in separate cell)\n",
        "# =========================\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import PeftModel\n",
        "import gc\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    base_model_name = \"meta-llama/Llama-3.2-3B\"\n",
        "    fine_tuned_dir = \"/content/llama3_policy_finetune_v2.5\"\n",
        "\n",
        "    bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_compute_dtype=torch.float16,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(fine_tuned_dir, trust_remote_code=True)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=bnb_config,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    model = PeftModel.from_pretrained(base_model, fine_tuned_dir)\n",
        "    model = model.merge_and_unload()\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_response(prompt, max_new_tokens=200):\n",
        "    model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    # torch.manual_seed(97)\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=True,\n",
        "            temperature=0.4,\n",
        "            top_k=20,\n",
        "            top_p=0.9,\n",
        "            repetition_penalty=1.2,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return response.strip()\n",
        "\n",
        "st.title(\"Llama-3 Policy Assistant\")\n",
        "prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "if st.button(\"Generate Response\"):\n",
        "    if prompt:\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        response = generate_response(prompt)\n",
        "        # Split into sentences and keep first 10\n",
        "        sentences = response.strip().split(\".\")\n",
        "        short_response = \".\".join(sentences[:10]).strip() + \".\"\n",
        "        st.write(short_response)\n",
        "    else:\n",
        "        st.warning(\"Please enter a question\")\n",
        "\n",
        "  # COPY PASTE THIS BEFORE YOUR PROMPT. You are a NACCAS policy expert. Answer using only official NACCAS documentation.\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oK1PnROdMdED",
        "outputId": "43ae2bc5-efe4-472f-ad60-a0ef4b8b32a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… MAIN EXECUTION (in separate cell)\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face\n",
        "login(token=\"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\")  # Replace with your token\n",
        "\n",
        "\n",
        "# Set ngrok authtoken\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")  # Replace with your token\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xjLQ7TFEMpF_",
        "outputId": "37c40dd8-7352-4756-c7d2-ae229c5eba10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://9e86-34-105-83-124.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.105.83.124:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-05-21 17:37:14.544330: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1747849034.799655    1596 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1747849034.862042    1596 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-21 17:37:15.406416: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2025-05-21 17:37:21.950 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Inference v3"
      ],
      "metadata": {
        "id": "xHuY5mbMbDfa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Significantly better and coherent results. requires minor adjustment, still putting out tokens like < /s> or < /item> etc"
      ],
      "metadata": {
        "id": "L_rTOZjViAxq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… SETUP & DEPENDENCIES\n",
        "# =========================\n",
        "!pip install -q unsloth peft transformers accelerate bitsandbytes streamlit pyngrok\n",
        "!pip install PyPDF2 docx\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YaTm9wwpbFvE",
        "outputId": "ee165d2d-d77b-4c1e-a050-643faea462e6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m491.5/491.5 kB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m146.6/146.6 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.1/79.1 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m31.5/31.5 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m865.2/865.2 MB\u001b[0m \u001b[31m854.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.5/156.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m91.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m50.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m125.3/125.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m70.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.0 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Collecting docx\n",
            "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m54.9/54.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from docx) (5.4.0)\n",
            "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.11/dist-packages (from docx) (11.2.1)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: docx\n",
            "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53893 sha256=7076cd51b5570e68b4ee08e9ded08a4d211f25c8d2a42c83ccb32952dd5f2960\n",
            "  Stored in directory: /root/.cache/pip/wheels/c1/3e/c3/e81c11effd0be5658a035947c66792dd993bcff317eae0e1ed\n",
            "Successfully built docx\n",
            "Installing collected packages: PyPDF2, docx\n",
            "Successfully installed PyPDF2-3.0.1 docx-0.2.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SMPLE STREAMLIT GUI\n",
        "\n",
        "# %%writefile app.py\n",
        "# import streamlit as st\n",
        "# import torch\n",
        "# from unsloth import FastLanguageModel\n",
        "# from transformers import AutoTokenizer\n",
        "# import gc\n",
        "# import re\n",
        "\n",
        "# @st.cache_resource\n",
        "# def load_model_and_tokenizer():\n",
        "#     fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "\n",
        "#     # Load the fine-tuned model and tokenizer\n",
        "#     model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "#         model_name=fine_tuned_dir,\n",
        "#         load_in_4bit=True,\n",
        "#         max_seq_length=2048,\n",
        "#         device_map=\"auto\",\n",
        "#     )\n",
        "#     model.eval()\n",
        "#     return model, tokenizer\n",
        "\n",
        "# def clean_response(response):\n",
        "#     # Remove all HTML-like tags, special tokens, and normalize whitespace\n",
        "#     response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)  # Remove tags like <item>, </item>, <s>, </s>, <span>, [INST], etc.\n",
        "#     response = re.sub(r'\\s+', ' ', response).strip()  # Normalize whitespace\n",
        "#     # Split into sentences and take the first 10 valid sentences\n",
        "#     sentences = response.split('.')\n",
        "#     cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "#     return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "# def generate_response(prompt, max_new_tokens=100):\n",
        "#     model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "# # ----------------------- Format the prompt with NACCAS context -------------------------\n",
        "#     formatted_prompt = (\n",
        "#         f\"<s>[INST] You are a NACCAS policy expert. Answer the following question based strictly on official NACCAS documentation: {prompt} [/INST]\"\n",
        "#     )\n",
        "#     input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model.generate(\n",
        "#             input_ids=input_ids,\n",
        "#             max_new_tokens=max_new_tokens,\n",
        "#             do_sample=True,\n",
        "#             temperature=0.3,  # Lowered for less randomness\n",
        "#             top_k=10,        # Stricter top-k for coherence\n",
        "#             top_p=0.85,      # Adjusted for more focused sampling\n",
        "#             repetition_penalty=1.3,  # Increased to reduce repetition\n",
        "#             eos_token_id=tokenizer.eos_token_id,\n",
        "#             pad_token_id=tokenizer.eos_token_id\n",
        "#         )\n",
        "\n",
        "#     response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "#     return clean_response(response)\n",
        "\n",
        "# st.title(\"NACCAS Policy Assistant\")\n",
        "# prompt = st.text_area(\"Ask about NACCAS policies:\")\n",
        "# if st.button(\"Generate Response\"):\n",
        "#     if prompt:\n",
        "#         gc.collect()\n",
        "#         torch.cuda.empty_cache()\n",
        "#         response = generate_response(prompt)\n",
        "#         st.write(response)\n",
        "#     else:\n",
        "#         st.warning(\"Please enter a question about NACCAS policies\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kScR0pugQLn",
        "outputId": "272271d1-c5af-488b-9f15-c1481603c06f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import PyPDF2\n",
        "try:\n",
        "    import docx\n",
        "except ImportError:\n",
        "    docx = None\n",
        "import uuid\n",
        "import json\n",
        "import os\n",
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "import re\n",
        "\n",
        "# Initialize session state\n",
        "if 'chat_sessions' not in st.session_state:\n",
        "    st.session_state.chat_sessions = {}\n",
        "if 'current_chat_id' not in st.session_state:\n",
        "    st.session_state.current_chat_id = str(uuid.uuid4())\n",
        "if 'memory' not in st.session_state:\n",
        "    st.session_state.memory = []\n",
        "if 'chat_names' not in st.session_state:\n",
        "    st.session_state.chat_names = {}\n",
        "\n",
        "# Load persistent state from JSON\n",
        "def load_state():\n",
        "    try:\n",
        "        with open(\"chat_state.json\", \"r\") as f:\n",
        "            state = json.load(f)\n",
        "            st.session_state.chat_sessions = state.get(\"chat_sessions\", {})\n",
        "            st.session_state.memory = state.get(\"memory\", [])\n",
        "            st.session_state.current_chat_id = state.get(\"current_chat_id\", str(uuid.uuid4()))\n",
        "            st.session_state.chat_names = state.get(\"chat_names\", {})\n",
        "    except FileNotFoundError:\n",
        "        pass\n",
        "\n",
        "def save_state():\n",
        "    with open(\"chat_state.json\", \"w\") as f:\n",
        "        json.dump({\n",
        "            \"chat_sessions\": st.session_state.chat_sessions,\n",
        "            \"memory\": st.session_state.memory,\n",
        "            \"current_chat_id\": st.session_state.current_chat_id,\n",
        "            \"chat_names\": st.session_state.chat_names\n",
        "        }, f)\n",
        "\n",
        "load_state()\n",
        "\n",
        "@st.cache_resource\n",
        "def load_model_and_tokenizer():\n",
        "    fine_tuned_dir = \"/content/drive/MyDrive/llama3.2-instruct-best\"\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=fine_tuned_dir,\n",
        "        max_seq_length=2048,\n",
        "        load_in_4bit=True,\n",
        "        device_map=\"auto\",\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model)\n",
        "    model.eval()\n",
        "    return model, tokenizer\n",
        "\n",
        "model, tokenizer = load_model_and_tokenizer()\n",
        "\n",
        "def clean_response(response):\n",
        "    response = re.sub(r'</?(item|s|span|INST|[^>]+)>', '', response)\n",
        "    response = re.sub(r'\\s+', ' ', response).strip()\n",
        "    sentences = response.split('.')\n",
        "    cleaned_sentences = [s.strip() for s in sentences if s.strip()]\n",
        "    return '. '.join(cleaned_sentences[:10]) + '.' if cleaned_sentences else ''\n",
        "\n",
        "def generate_response(prompt, chat_history=None):\n",
        "    formatted_prompt = f\"<s>[INST] You are a NACCAS policy expert. Answer the following based strictly on official NACCAS documentation:\\nUser: {prompt} [/INST]\"\n",
        "\n",
        "    input_ids = tokenizer(formatted_prompt, return_tensors=\"pt\").input_ids.to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=input_ids,\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.3,\n",
        "            top_k=10,\n",
        "            top_p=0.85,\n",
        "            repetition_penalty=1.3,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "\n",
        "    response = tokenizer.decode(outputs[0][input_ids.shape[1]:], skip_special_tokens=True)\n",
        "    return clean_response(response)\n",
        "\n",
        "def extract_pdf_text(file):\n",
        "    try:\n",
        "        pdf_reader = PyPDF2.PdfReader(file)\n",
        "        text = \"\"\n",
        "        for page in pdf_reader.pages:\n",
        "            text += page.extract_text() or \"\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading PDF: {str(e)}\"\n",
        "\n",
        "def extract_docx_text(file):\n",
        "    if docx is None:\n",
        "        return \"Word document support unavailable (python-docx not installed).\"\n",
        "    try:\n",
        "        doc = docx.Document(file)\n",
        "        text = \"\"\n",
        "        for para in doc.paragraphs:\n",
        "            text += para.text + \"\\n\"\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        return f\"Error reading Word document: {str(e)}\"\n",
        "\n",
        "def process_uploaded_file(uploaded_file):\n",
        "    if uploaded_file is not None:\n",
        "        if uploaded_file.name.endswith('.pdf'):\n",
        "            text = extract_pdf_text(uploaded_file)\n",
        "        elif uploaded_file.name.endswith('.docx'):\n",
        "            text = extract_docx_text(uploaded_file)\n",
        "        else:\n",
        "            text = \"Unsupported file format. Please upload a PDF or Word document.\"\n",
        "        return text\n",
        "    return None\n",
        "\n",
        "def new_chat():\n",
        "    new_chat_id = str(uuid.uuid4())\n",
        "    st.session_state.chat_sessions[new_chat_id] = []\n",
        "    st.session_state.current_chat_id = new_chat_id\n",
        "    if len(st.session_state.chat_sessions) > 5:\n",
        "        oldest_chat_id = next(iter(st.session_state.chat_sessions))\n",
        "        del st.session_state.chat_sessions[oldest_chat_id]\n",
        "        if oldest_chat_id in st.session_state.chat_names:\n",
        "            del st.session_state.chat_names[oldest_chat_id]\n",
        "    save_state()\n",
        "\n",
        "def update_chat_name(chat_id, message):\n",
        "    if chat_id not in st.session_state.chat_names and message[\"role\"] == \"user\":\n",
        "        words = message[\"content\"].split()[:5]\n",
        "        st.session_state.chat_names[chat_id] = \" \".join(words) + \" ...\"\n",
        "        save_state()\n",
        "\n",
        "# ---------- UI ----------\n",
        "\n",
        "# CSS Styles\n",
        "st.markdown(\"\"\"\n",
        "<style>\n",
        ".sidebar .sidebar-content {\n",
        "    width: 200px !important;\n",
        "}\n",
        ".css-1vq4p4l.e1f1d6gn2 {\n",
        "    padding-bottom: 90px !important;\n",
        "}\n",
        "div[data-testid=\"stForm\"] input[type=\"text\"] {\n",
        "    width: 100% !important;\n",
        "    max-width: 500px !important;\n",
        "}\n",
        "\n",
        "input[type=\"text\"] {\n",
        "    font-size: 0.85rem !important;\n",
        "    height: 2.2rem !important;\n",
        "    padding: 0.3rem 0.5rem !important;\n",
        "    width: 100% !important;\n",
        "}\n",
        "button[kind=\"primary\"] {\n",
        "    font-size: 0.85rem !important;\n",
        "    padding: 0.3rem 1rem !important;\n",
        "}\n",
        "</style>\n",
        "\"\"\", unsafe_allow_html=True)\n",
        "\n",
        "# Sidebar\n",
        "st.sidebar.title(\"Chat Sessions\")\n",
        "st.sidebar.button(\"New Chat\", on_click=new_chat)\n",
        "\n",
        "for chat_id in list(st.session_state.chat_sessions.keys())[:5]:\n",
        "    chat_name = st.session_state.chat_names.get(chat_id, f\"Chat {chat_id[:8]}\")\n",
        "    if st.sidebar.button(chat_name, key=chat_id):\n",
        "        st.session_state.current_chat_id = chat_id\n",
        "        save_state()\n",
        "\n",
        "# Main Title\n",
        "st.title(\"NACCAS Policy Assistant\")\n",
        "\n",
        "# Chat history\n",
        "if st.session_state.current_chat_id not in st.session_state.chat_sessions:\n",
        "    st.session_state.chat_sessions[st.session_state.current_chat_id] = []\n",
        "    save_state()\n",
        "\n",
        "st.subheader(\"Chat History\")\n",
        "for message in st.session_state.chat_sessions[st.session_state.current_chat_id]:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.write(message[\"content\"])\n",
        "    if message[\"role\"] == \"user\" and st.session_state.current_chat_id not in st.session_state.chat_names:\n",
        "        update_chat_name(st.session_state.current_chat_id, message)\n",
        "\n",
        "# Fixed Input Bar Form\n",
        "with st.form(key=\"input_form\", clear_on_submit=True):\n",
        "    upload_col, input_col, button_col = st.columns([1, 3, 1])\n",
        "    with upload_col:\n",
        "        uploaded_file = st.file_uploader(\"\", type=['pdf', 'docx'], label_visibility=\"collapsed\")\n",
        "    with input_col:\n",
        "        user_input = st.text_input(\"\", placeholder=\"Ask about NACCAS policies...\", key=\"user_input\")\n",
        "    with button_col:\n",
        "        submit_button = st.form_submit_button(\"Send\")\n",
        "\n",
        "if submit_button and (user_input or uploaded_file):\n",
        "    current_chat = st.session_state.chat_sessions[st.session_state.current_chat_id]\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    if uploaded_file:\n",
        "        file_content = process_uploaded_file(uploaded_file)\n",
        "        if file_content:\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Uploaded file content:\\n{file_content}\"\n",
        "            })\n",
        "            update_chat_name(st.session_state.current_chat_id, {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": f\"Uploaded file content:\\n{file_content}\"\n",
        "            })\n",
        "            assistant_response = generate_response(f\"User uploaded a file: {uploaded_file.name}. Content: {file_content}\", current_chat)\n",
        "            st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": assistant_response\n",
        "            })\n",
        "            save_state()\n",
        "\n",
        "    if user_input:\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        })\n",
        "        update_chat_name(st.session_state.current_chat_id, {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": user_input\n",
        "        })\n",
        "        assistant_response = generate_response(user_input, current_chat)\n",
        "        st.session_state.chat_sessions[st.session_state.current_chat_id].append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": assistant_response\n",
        "        })\n",
        "        save_state()\n",
        "\n",
        "    st.rerun()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4aKE0HgGNu2",
        "outputId": "6b7ee632-6f3f-442e-bba7-bd1906a8696d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================\n",
        "# âœ… MAIN EXECUTION\n",
        "# =========================\n",
        "import os\n",
        "from huggingface_hub import login\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Authenticate with Hugging Face (replace with your token)\n",
        "login(token=\"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\")\n",
        "\n",
        "# Set ngrok authtoken (replace with your token)\n",
        "ngrok.set_auth_token(\"2wqBxne2LxuZWd50w0W38bok7w4_7uoyrn8XJW52aaEfcgjn8\")\n",
        "\n",
        "# Start Streamlit with ngrok\n",
        "public_url = ngrok.connect(8501)\n",
        "print(\"Public URL:\", public_url)\n",
        "!streamlit run app.py --server.port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0r4Mm2ADgOjv",
        "outputId": "49e4249c-e89a-4e27-aa3e-b6cbe5f83a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://ff52-34-168-209-159.ngrok-free.app\" -> \"http://localhost:8501\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.168.209.159:8501\u001b[0m\n",
            "\u001b[0m\n",
            "2025-05-27 17:25:58.745 Uncaught exception GET /_stcore/stream (127.0.0.1)\n",
            "HTTPServerRequest(protocol='http', host='ff52-34-168-209-159.ngrok-free.app', method='GET', uri='/_stcore/stream', version='HTTP/1.1', remote_ip='127.0.0.1')\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/websocket.py\", line 938, in _accept_connection\n",
            "    open_result = handler.open(*handler.open_args, **handler.open_kwargs)\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/browser_websocket_handler.py\", line 177, in open\n",
            "    self._session_id = self._runtime.connect_session(\n",
            "                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 384, in connect_session\n",
            "    session_id = self._session_mgr.connect_session(\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/websocket_session_manager.py\", line 99, in connect_session\n",
            "    session = AppSession(\n",
            "              ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 158, in __init__\n",
            "    self.register_file_watchers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 195, in register_file_watchers\n",
            "    self._local_sources_watcher = LocalSourcesWatcher(self._pages_manager)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 65, in __init__\n",
            "    self.update_watched_pages()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 77, in update_watched_pages\n",
            "    self._register_watcher(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 136, in _register_watcher\n",
            "    watcher=PathWatcher(filepath, self.on_file_changed),\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 107, in __init__\n",
            "    path_watcher.watch_path(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/event_based_path_watcher.py\", line 185, in watch_path\n",
            "    folder_handler.watch = self._observer.schedule(\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/api.py\", line 312, in schedule\n",
            "    emitter.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/utils/__init__.py\", line 75, in start\n",
            "    self.on_thread_start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify.py\", line 119, in on_thread_start\n",
            "    self._inotify = InotifyBuffer(path, recursive=self.watch.is_recursive, event_mask=event_mask)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_buffer.py\", line 30, in __init__\n",
            "    self._inotify = Inotify(path, recursive=recursive, event_mask=event_mask)\n",
            "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 185, in __init__\n",
            "    self._add_dir_watch(path, event_mask, recursive=recursive)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 411, in _add_dir_watch\n",
            "    self._add_watch(full_path, mask)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 424, in _add_watch\n",
            "    Inotify._raise_error()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/watchdog/observers/inotify_c.py\", line 441, in _raise_error\n",
            "    raise OSError(err, os.strerror(err))\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Exception ignored in: <function AppSession.__del__ at 0x78bfe8e7bc40>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 178, in __del__\n",
            "    self.shutdown()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 255, in shutdown\n",
            "    self.request_script_stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/app_session.py\", line 428, in request_script_stop\n",
            "    if self._scriptrunner is not None:\n",
            "       ^^^^^^^^^^^^^^^^^^\n",
            "AttributeError: 'AppSession' object has no attribute '_scriptrunner'\n",
            "ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "2025-05-27 17:26:06.016985: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748366766.040699    5310 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748366766.047921    5310 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-05-27 17:26:06.070884: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2025.5.7: Fast Llama patching. Transformers: 4.51.3.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.7.0+cu126. CUDA: 7.5. CUDA Toolkit: 12.6. Triton: 3.3.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.30. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
            "Unsloth 2025.5.7 patched 28 layers with 28 QKV layers, 28 O layers and 28 MLP layers.\n",
            "2025-05-27 17:26:29.004 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:26:29.005 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:26:30.060 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-05-27 17:26:52.140 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:26:52.141 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "2025-05-27 17:27:04.315 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:04.316 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:05.424 Examining the path of torch.classes raised:\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 347, in run\n",
            "    if asyncio.get_running_loop().is_running():\n",
            "       ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: no running event loop\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 217, in get_module_paths\n",
            "    potential_paths = extract_paths(module)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/watcher/local_sources_watcher.py\", line 210, in <lambda>\n",
            "    lambda m: list(m.__path__._path),\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_classes.py\", line 13, in __getattr__\n",
            "    proxy = torch._C._get_custom_class_python_wrapper(self.name, attr)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: Tried to instantiate class '__path__._path', but it does not exist! Ensure that it is registered via torch::class_\n",
            "2025-05-27 17:27:53.992 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:53.993 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:58.328 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n",
            "2025-05-27 17:27:58.329 `label` got an empty value. This is discouraged for accessibility reasons and may be disallowed in the future by raising an exception. Please provide a non-empty label and hide it with label_visibility if needed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2025-05-27T17:28:31+0000 lvl=warn msg=\"Stopping forwarder\" name=http-8501-7d785337-271d-4259-bd43-c41aa4b8fca2 acceptErr=\"failed to accept connection: Listener closed\"\n",
            "WARNING:pyngrok.process.ngrok:t=2025-05-27T17:28:31+0000 lvl=warn msg=\"Error restarting forwarder\" name=http-8501-7d785337-271d-4259-bd43-c41aa4b8fca2 err=\"failed to start tunnel: session closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Exception ignored in atexit callback: <function shutdown_compile_workers at 0x78be4cd08e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/async_compile.py\", line 113, in shutdown_compile_workers\n",
            "    pool.shutdown()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 237, in shutdown\n",
            "    _send_msg(self.write_pipe, -1)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_inductor/compile_worker/subproc_pool.py\", line 51, in _send_msg\n",
            "    write_pipe.flush()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/bootstrap.py\", line 44, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/web/server/server.py\", line 469, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/streamlit/runtime/runtime.py\", line 324, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 807, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 520, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ]
    }
  ]
}